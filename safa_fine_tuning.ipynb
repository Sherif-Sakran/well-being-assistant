{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "365d0200e3c34df8be25ee24c1c38b31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ac0cd713c6584ba6940596f28ba81552",
              "IPY_MODEL_f2e9e9adfee94a599111d659bbcf724b",
              "IPY_MODEL_213626f28ec7473da51b2ac309954ad1"
            ],
            "layout": "IPY_MODEL_c57a6f88f92f4d1bbb2bd1a1b59c4645"
          }
        },
        "ac0cd713c6584ba6940596f28ba81552": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bec9b848b5f048e7a607e4819fc3ebe9",
            "placeholder": "​",
            "style": "IPY_MODEL_7a7854cdda2646a89b5b6a6ef0ed1487",
            "value": "model.safetensors: 100%"
          }
        },
        "f2e9e9adfee94a599111d659bbcf724b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2e210894dfdb4a43a18ad493acbb5c21",
            "max": 5702746403,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b4ad82bc05a14db2b012e265115dda2c",
            "value": 5702746403
          }
        },
        "213626f28ec7473da51b2ac309954ad1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ac15a3571d8240a49aaa448a3ffc6236",
            "placeholder": "​",
            "style": "IPY_MODEL_56ae158e846e4e8485c6909a46eeb7d0",
            "value": " 5.70G/5.70G [01:00&lt;00:00, 226MB/s]"
          }
        },
        "c57a6f88f92f4d1bbb2bd1a1b59c4645": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bec9b848b5f048e7a607e4819fc3ebe9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a7854cdda2646a89b5b6a6ef0ed1487": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2e210894dfdb4a43a18ad493acbb5c21": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b4ad82bc05a14db2b012e265115dda2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ac15a3571d8240a49aaa448a3ffc6236": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "56ae158e846e4e8485c6909a46eeb7d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a15bc17e1d75487c966042272b41fb48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_832b7729882e4d09979d9498e1e8d78c",
              "IPY_MODEL_406d1cfba7a444078dee2d56f8d935e2",
              "IPY_MODEL_6965e3e0072b4d3c9f46f83f3b3ad2f1"
            ],
            "layout": "IPY_MODEL_e6865022831d49728ef49196cafc1137"
          }
        },
        "832b7729882e4d09979d9498e1e8d78c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6e77dac2f49b4d1ea4ee788105a86fd5",
            "placeholder": "​",
            "style": "IPY_MODEL_1a7dd6f9dfa94fe7bde428030dc6bbac",
            "value": "generation_config.json: 100%"
          }
        },
        "406d1cfba7a444078dee2d56f8d935e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4d36e1a53cd344c28e5034e5c46330ee",
            "max": 220,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2c5811397b364d7e8c54a83c60d5667a",
            "value": 220
          }
        },
        "6965e3e0072b4d3c9f46f83f3b3ad2f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_da656a1737c041a98c799e0722269b81",
            "placeholder": "​",
            "style": "IPY_MODEL_045ad37ba75541249bc04eab99d6c997",
            "value": " 220/220 [00:00&lt;00:00, 7.09kB/s]"
          }
        },
        "e6865022831d49728ef49196cafc1137": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e77dac2f49b4d1ea4ee788105a86fd5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1a7dd6f9dfa94fe7bde428030dc6bbac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4d36e1a53cd344c28e5034e5c46330ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c5811397b364d7e8c54a83c60d5667a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "da656a1737c041a98c799e0722269b81": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "045ad37ba75541249bc04eab99d6c997": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "17717f811b6a46f7a6fea265f81088ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_73994fa586c045d6bf86f8197ff22f93",
              "IPY_MODEL_8061057b064949e78a2f03fed851e8bf",
              "IPY_MODEL_7026a5fed8924382949d7a687f0288c4"
            ],
            "layout": "IPY_MODEL_8a5d680dbc3e4fd68f5817d7fd630c01"
          }
        },
        "73994fa586c045d6bf86f8197ff22f93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_13a33036e3894b2ead6ccaf9acd8858d",
            "placeholder": "​",
            "style": "IPY_MODEL_847948463f27450a83bd097b6d9421b4",
            "value": "tokenizer_config.json: "
          }
        },
        "8061057b064949e78a2f03fed851e8bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0027ccb60e5945649da77dea4c81affb",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4bdc5434bc30483fbbc3f3827dbd9743",
            "value": 1
          }
        },
        "7026a5fed8924382949d7a687f0288c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_86cb0d61a8274efca911381236ec3602",
            "placeholder": "​",
            "style": "IPY_MODEL_690d2543a77f4a87a228cbf4fee6b462",
            "value": " 51.1k/? [00:00&lt;00:00, 1.61MB/s]"
          }
        },
        "8a5d680dbc3e4fd68f5817d7fd630c01": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "13a33036e3894b2ead6ccaf9acd8858d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "847948463f27450a83bd097b6d9421b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0027ccb60e5945649da77dea4c81affb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "4bdc5434bc30483fbbc3f3827dbd9743": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "86cb0d61a8274efca911381236ec3602": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "690d2543a77f4a87a228cbf4fee6b462": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aeeeb97317d5479bb82ddcf6e5697d25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1ff6c7044a904072b81902bedffdf357",
              "IPY_MODEL_1d7a80d65a354001b2d807569d5d38e9",
              "IPY_MODEL_37c16a9b8da041639ba4dd059edd242e"
            ],
            "layout": "IPY_MODEL_0430f561904e4e43923d0c6a39e0b811"
          }
        },
        "1ff6c7044a904072b81902bedffdf357": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8fd02d8d32ee427885be53a1ce2ef874",
            "placeholder": "​",
            "style": "IPY_MODEL_a3170073d88749eea0bb6a6194424449",
            "value": "tokenizer.json: "
          }
        },
        "1d7a80d65a354001b2d807569d5d38e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2e26204d100a44a0b13edbd730726d5e",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_835065177d55443f8ce11b6ff4c5b8ed",
            "value": 1
          }
        },
        "37c16a9b8da041639ba4dd059edd242e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b908cae3872421a9ae6c31bf741799c",
            "placeholder": "​",
            "style": "IPY_MODEL_f8b9bd13fa3a4a1a9795da37d76d2917",
            "value": " 9.09M/? [00:00&lt;00:00, 26.9MB/s]"
          }
        },
        "0430f561904e4e43923d0c6a39e0b811": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8fd02d8d32ee427885be53a1ce2ef874": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a3170073d88749eea0bb6a6194424449": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2e26204d100a44a0b13edbd730726d5e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "835065177d55443f8ce11b6ff4c5b8ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3b908cae3872421a9ae6c31bf741799c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f8b9bd13fa3a4a1a9795da37d76d2917": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "25ef13c26f19433fa766a6773d7b617d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fe100ba60d8543b4a272bb6eaa2ff4dc",
              "IPY_MODEL_d73b7c0dd1eb4048838dbe374e6f3862",
              "IPY_MODEL_f20b8111a7014f5a91d2e7e74966a639"
            ],
            "layout": "IPY_MODEL_e01d4fcda2ac46b88d8981aca78cf66b"
          }
        },
        "fe100ba60d8543b4a272bb6eaa2ff4dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_15020033d493486c8baff8de47f96cdf",
            "placeholder": "​",
            "style": "IPY_MODEL_4226e3714e1a4e3892e9a2c376a6c225",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "d73b7c0dd1eb4048838dbe374e6f3862": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_69446aa220754f94a59b425f85b93899",
            "max": 345,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_03bbca3f71e043d5ac8abbf93c48f201",
            "value": 345
          }
        },
        "f20b8111a7014f5a91d2e7e74966a639": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1a9a95f3eb7b4191a6be5f7204f597ba",
            "placeholder": "​",
            "style": "IPY_MODEL_86aa122813e3457da85b8e7edae47f15",
            "value": " 345/345 [00:00&lt;00:00, 16.7kB/s]"
          }
        },
        "e01d4fcda2ac46b88d8981aca78cf66b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "15020033d493486c8baff8de47f96cdf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4226e3714e1a4e3892e9a2c376a6c225": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "69446aa220754f94a59b425f85b93899": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "03bbca3f71e043d5ac8abbf93c48f201": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1a9a95f3eb7b4191a6be5f7204f597ba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "86aa122813e3457da85b8e7edae47f15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "45c60d0a59fe4fe2ae04f88878e8694c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3fc4a5eeb9924827b226dacb9eab84ae",
              "IPY_MODEL_a500b3179a174c14aa61bb577d7a5d33",
              "IPY_MODEL_c9d17ebabd08481390785d7d3e11ed9f"
            ],
            "layout": "IPY_MODEL_b8a197f2655c4c5f8728fc3dc02d9e36"
          }
        },
        "3fc4a5eeb9924827b226dacb9eab84ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0b632ae327bc40269900ad2b6a2ea9b3",
            "placeholder": "​",
            "style": "IPY_MODEL_f2043d9ca26f4633be1d5b734325d699",
            "value": "Unsloth: Tokenizing [&quot;text&quot;] (num_proc=6): 100%"
          }
        },
        "a500b3179a174c14aa61bb577d7a5d33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f8e0ea4aaa8f4aabb9e59e1d207b9225",
            "max": 100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_450d55d5227446dfbf6bc4d9b4831ea3",
            "value": 100
          }
        },
        "c9d17ebabd08481390785d7d3e11ed9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_01457646a85c4b51a9b9452e4bbd56e1",
            "placeholder": "​",
            "style": "IPY_MODEL_0a01bcbec6fb48cd902eb485116c2b20",
            "value": " 100/100 [00:06&lt;00:00, 24.51 examples/s]"
          }
        },
        "b8a197f2655c4c5f8728fc3dc02d9e36": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0b632ae327bc40269900ad2b6a2ea9b3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f2043d9ca26f4633be1d5b734325d699": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f8e0ea4aaa8f4aabb9e59e1d207b9225": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "450d55d5227446dfbf6bc4d9b4831ea3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "01457646a85c4b51a9b9452e4bbd56e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0a01bcbec6fb48cd902eb485116c2b20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ceb080df29494f64b78358b411b0e987": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1eaec69766954bb69a6298a891359f5c",
              "IPY_MODEL_70dd6c27ab1c4d979a85918c7dcd803c",
              "IPY_MODEL_5079dd9bb4694e448ededa2304920593"
            ],
            "layout": "IPY_MODEL_5fa7f9cd91664222bbf6df0b1f6d5ee0"
          }
        },
        "1eaec69766954bb69a6298a891359f5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_505a4fd824164ebbb6ed52f276208c72",
            "placeholder": "​",
            "style": "IPY_MODEL_293e068e9c4e444192036bd72ab93904",
            "value": "config.json: 100%"
          }
        },
        "70dd6c27ab1c4d979a85918c7dcd803c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_17cd86844f9c4ae199c52dd205e8e539",
            "max": 777,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cd5e364eed5743a199ebb9dab1f109d9",
            "value": 777
          }
        },
        "5079dd9bb4694e448ededa2304920593": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aef3bca00d3342848300f7a7025c3cbb",
            "placeholder": "​",
            "style": "IPY_MODEL_c4619ba7d7b349c3ae5870bd3c211e71",
            "value": " 777/777 [00:00&lt;00:00, 79.2kB/s]"
          }
        },
        "5fa7f9cd91664222bbf6df0b1f6d5ee0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "505a4fd824164ebbb6ed52f276208c72": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "293e068e9c4e444192036bd72ab93904": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "17cd86844f9c4ae199c52dd205e8e539": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cd5e364eed5743a199ebb9dab1f109d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "aef3bca00d3342848300f7a7025c3cbb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c4619ba7d7b349c3ae5870bd3c211e71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7bc0027616104100a85f46bb232241c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_39a4c73635b44e03932518d620b65379",
              "IPY_MODEL_74f16b1fe19940dea6d7b01810ef97df",
              "IPY_MODEL_c52e48a67eca4e5697000d509fa5e3a4"
            ],
            "layout": "IPY_MODEL_059efc601d6c41a993bd68c1e2eb3f7d"
          }
        },
        "39a4c73635b44e03932518d620b65379": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_979944fa35e94d7a8429c1420ace9470",
            "placeholder": "​",
            "style": "IPY_MODEL_b2f7c5da27044983a607bbad6acf0735",
            "value": "Fetching 1 files: 100%"
          }
        },
        "74f16b1fe19940dea6d7b01810ef97df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7ce6d3f0e53c4fdf92b62532377797f0",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ca15d9b413764b7985352456939ff0be",
            "value": 1
          }
        },
        "c52e48a67eca4e5697000d509fa5e3a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5366bd6f22284095b9368231ebf23c2a",
            "placeholder": "​",
            "style": "IPY_MODEL_6b44173aef4f4a379ba0fe5d0ee57146",
            "value": " 1/1 [00:00&lt;00:00,  2.25it/s]"
          }
        },
        "059efc601d6c41a993bd68c1e2eb3f7d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "979944fa35e94d7a8429c1420ace9470": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b2f7c5da27044983a607bbad6acf0735": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7ce6d3f0e53c4fdf92b62532377797f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca15d9b413764b7985352456939ff0be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5366bd6f22284095b9368231ebf23c2a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b44173aef4f4a379ba0fe5d0ee57146": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1a611f3c86324e99a3e1afd56c1d98d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f88a46db78564ff3bbeaad713adf10df",
              "IPY_MODEL_11b18478cc2042fa82801c945cbe4b4b",
              "IPY_MODEL_64656fef6e77449baa880b04102d7311"
            ],
            "layout": "IPY_MODEL_9e22a4c95db048b89db39613378a090a"
          }
        },
        "f88a46db78564ff3bbeaad713adf10df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a526812054d94197a272841c7e78a372",
            "placeholder": "​",
            "style": "IPY_MODEL_73fdc5b22a8e4697b2c260d503a28da0",
            "value": "model.safetensors.index.json: "
          }
        },
        "11b18478cc2042fa82801c945cbe4b4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_003be1d3c92e4f589b9328928e8d8ebd",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1f3a45a86dd246c492a02f3acba035e3",
            "value": 1
          }
        },
        "64656fef6e77449baa880b04102d7311": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8a377cba540940edb46921728abbc0a4",
            "placeholder": "​",
            "style": "IPY_MODEL_9a5eaea9ad9c494b9b279d5402c2f946",
            "value": " 23.9k/? [00:00&lt;00:00, 783kB/s]"
          }
        },
        "9e22a4c95db048b89db39613378a090a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a526812054d94197a272841c7e78a372": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "73fdc5b22a8e4697b2c260d503a28da0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "003be1d3c92e4f589b9328928e8d8ebd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "1f3a45a86dd246c492a02f3acba035e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8a377cba540940edb46921728abbc0a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a5eaea9ad9c494b9b279d5402c2f946": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "439789527c0c45629710f1731c7f1ebd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9ed6db4a1f654ed085d33b42cc7058a2",
              "IPY_MODEL_98e86d708c984395b7664ced757348cd",
              "IPY_MODEL_1bdb457cdbef439c880b6fea28ab3230"
            ],
            "layout": "IPY_MODEL_43a0c2186f194905a429d00e52f3d0b9"
          }
        },
        "9ed6db4a1f654ed085d33b42cc7058a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ed259152ea914861895fc23d29c65c6f",
            "placeholder": "​",
            "style": "IPY_MODEL_4c39fc5939304431b14b9167c66ce410",
            "value": "model-00001-of-00004.safetensors: 100%"
          }
        },
        "98e86d708c984395b7664ced757348cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bed510c8f71347bfb29c4787441bd9e6",
            "max": 4976698672,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8253b83d76cb466cb1ccd4ef14c9315c",
            "value": 4976698672
          }
        },
        "1bdb457cdbef439c880b6fea28ab3230": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_28770ec4eb984adfb948f805c18d1396",
            "placeholder": "​",
            "style": "IPY_MODEL_c0c2a007e1bc477398a4a46ce4c10376",
            "value": " 4.98G/4.98G [17:37&lt;00:00, 10.2MB/s]"
          }
        },
        "43a0c2186f194905a429d00e52f3d0b9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed259152ea914861895fc23d29c65c6f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c39fc5939304431b14b9167c66ce410": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bed510c8f71347bfb29c4787441bd9e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8253b83d76cb466cb1ccd4ef14c9315c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "28770ec4eb984adfb948f805c18d1396": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c0c2a007e1bc477398a4a46ce4c10376": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ef16d4263b0a4dd3ae280ff424d2d64e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ce585d84e1554975a13f5f8cb8378ece",
              "IPY_MODEL_16c3e58579f34f40a3b50d3349755814",
              "IPY_MODEL_4ae4f535f58f44db9345cc324d5d317a"
            ],
            "layout": "IPY_MODEL_f469a6f38bed41aead079a98421eb953"
          }
        },
        "ce585d84e1554975a13f5f8cb8378ece": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2393cc37c1e64c9394831cc528563f0e",
            "placeholder": "​",
            "style": "IPY_MODEL_7c745ee261e243988a25f2c607bb61db",
            "value": "model-00002-of-00004.safetensors: 100%"
          }
        },
        "16c3e58579f34f40a3b50d3349755814": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1a5a1beb33254a4abb9800a4f4e1954b",
            "max": 4999802720,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dc0d12133138440b9ebd03d9e8222fef",
            "value": 4999802720
          }
        },
        "4ae4f535f58f44db9345cc324d5d317a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fef1447258d644da9c4cc45ea77e54c2",
            "placeholder": "​",
            "style": "IPY_MODEL_8a152f3b4d3c47df9624b0eb2bcef51e",
            "value": " 5.00G/5.00G [04:04&lt;00:00, 29.0MB/s]"
          }
        },
        "f469a6f38bed41aead079a98421eb953": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2393cc37c1e64c9394831cc528563f0e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c745ee261e243988a25f2c607bb61db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1a5a1beb33254a4abb9800a4f4e1954b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc0d12133138440b9ebd03d9e8222fef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fef1447258d644da9c4cc45ea77e54c2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a152f3b4d3c47df9624b0eb2bcef51e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "482b72e3789c444d8ae5d46e3ae64970": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e929b8322d64418a84c5b350744c9580",
              "IPY_MODEL_1cedb43ac49b40c7adf52bd2647157f9",
              "IPY_MODEL_5e12bbaed3254d4190813ff10ac7bae3"
            ],
            "layout": "IPY_MODEL_d39c145a68114ed18dd0519d9effe522"
          }
        },
        "e929b8322d64418a84c5b350744c9580": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3094af5b394a4c0d84ff9baeb1526ff6",
            "placeholder": "​",
            "style": "IPY_MODEL_4e420d1690c245f5a062e67573dd0070",
            "value": "model-00003-of-00004.safetensors: 100%"
          }
        },
        "1cedb43ac49b40c7adf52bd2647157f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_855cecd94aca4d0e892c223de365ea1d",
            "max": 4915916176,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_78884c5d2a2c40bc9f8b91518c0d07c8",
            "value": 4915916176
          }
        },
        "5e12bbaed3254d4190813ff10ac7bae3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_77325456fab342b8a6c69e483ee03527",
            "placeholder": "​",
            "style": "IPY_MODEL_933d75eea9dc4c6bb7fd4a9102fba5aa",
            "value": " 4.92G/4.92G [03:20&lt;00:00, 44.7MB/s]"
          }
        },
        "d39c145a68114ed18dd0519d9effe522": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3094af5b394a4c0d84ff9baeb1526ff6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e420d1690c245f5a062e67573dd0070": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "855cecd94aca4d0e892c223de365ea1d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "78884c5d2a2c40bc9f8b91518c0d07c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "77325456fab342b8a6c69e483ee03527": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "933d75eea9dc4c6bb7fd4a9102fba5aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "305aadf37840455797e993d4efb1b689": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e0f40d075fe242898ac09f1c36b5ebcb",
              "IPY_MODEL_cb05b30a5bff406e8e2a2a960a26cd76",
              "IPY_MODEL_fb654907f1834a1588ef43d2478fe30d"
            ],
            "layout": "IPY_MODEL_1f055d8db9004a96a8e795ffd1afde66"
          }
        },
        "e0f40d075fe242898ac09f1c36b5ebcb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9c816983d3e043aba7e7b7fb19a7386d",
            "placeholder": "​",
            "style": "IPY_MODEL_5cac21250d484228be5500703869441a",
            "value": "model-00004-of-00004.safetensors: 100%"
          }
        },
        "cb05b30a5bff406e8e2a2a960a26cd76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_37128cfe7a564aa487dfb1c3539c5ae2",
            "max": 1168138808,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cdbe58da68554ce5bb37fb77505346df",
            "value": 1168138808
          }
        },
        "fb654907f1834a1588ef43d2478fe30d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2091645759784cb78845a9b5cefcad58",
            "placeholder": "​",
            "style": "IPY_MODEL_11f63e3d83054b9282011e04fe6500df",
            "value": " 1.17G/1.17G [00:50&lt;00:00, 18.9MB/s]"
          }
        },
        "1f055d8db9004a96a8e795ffd1afde66": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c816983d3e043aba7e7b7fb19a7386d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5cac21250d484228be5500703869441a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "37128cfe7a564aa487dfb1c3539c5ae2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cdbe58da68554ce5bb37fb77505346df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2091645759784cb78845a9b5cefcad58": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "11f63e3d83054b9282011e04fe6500df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "BASE_DIR = '/content/'"
      ],
      "metadata": {
        "id": "VtTas0oJXsuf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DjfM-gTVWWWe"
      },
      "outputs": [],
      "source": [
        "# # Install Unsloth and its dependencies with xformers for memory efficiency\n",
        "# !pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "# !pip install --no-deps xformers \"trl<0.9.0\" peft accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unsloth trl peft accelerate bitsandbytes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VYKLYRlHBQdn",
        "outputId": "3692f47d-e541-4859-b35c-de24b14dc370"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting unsloth\n",
            "  Downloading unsloth-2025.9.5-py3-none-any.whl.metadata (54 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/54.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting trl\n",
            "  Downloading trl-0.23.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (0.17.1)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.10.1)\n",
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.47.0-py3-none-manylinux_2_24_x86_64.whl.metadata (11 kB)\n",
            "Collecting unsloth_zoo>=2025.9.6 (from unsloth)\n",
            "  Downloading unsloth_zoo-2025.9.6-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: torch>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from unsloth) (2.8.0+cu126)\n",
            "Collecting xformers>=0.0.27.post2 (from unsloth)\n",
            "  Downloading xformers-0.0.32.post2-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: triton>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from unsloth) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from unsloth) (25.0)\n",
            "Collecting tyro (from unsloth)\n",
            "  Downloading tyro-0.9.31-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: transformers!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,>=4.51.3 in /usr/local/lib/python3.12/dist-packages (from unsloth) (4.56.1)\n",
            "Collecting datasets<4.0.0,>=3.4.1 (from unsloth)\n",
            "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: sentencepiece>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.2.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from unsloth) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from unsloth) (5.9.5)\n",
            "Requirement already satisfied: wheel>=0.42.0 in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.45.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from unsloth) (2.0.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from unsloth) (5.29.5)\n",
            "Requirement already satisfied: huggingface_hub>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.34.4)\n",
            "Requirement already satisfied: hf_transfer in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.1.9)\n",
            "Requirement already satisfied: diffusers in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.35.1)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.23.0+cu126)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from peft) (6.0.2)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from peft) (0.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets<4.0.0,>=3.4.1->unsloth) (3.19.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets<4.0.0,>=3.4.1->unsloth) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets<4.0.0,>=3.4.1->unsloth) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets<4.0.0,>=3.4.1->unsloth) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets<4.0.0,>=3.4.1->unsloth) (2.32.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets<4.0.0,>=3.4.1->unsloth) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets<4.0.0,>=3.4.1->unsloth) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.34.0->unsloth) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.34.0->unsloth) (1.1.9)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (1.11.1.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,>=4.51.3->unsloth) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,>=4.51.3->unsloth) (0.22.0)\n",
            "Requirement already satisfied: torchao in /usr/local/lib/python3.12/dist-packages (from unsloth_zoo>=2025.9.6->unsloth) (0.10.0)\n",
            "Collecting cut_cross_entropy (from unsloth_zoo>=2025.9.6->unsloth)\n",
            "  Downloading cut_cross_entropy-25.1.1-py3-none-any.whl.metadata (9.3 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from unsloth_zoo>=2025.9.6->unsloth) (11.3.0)\n",
            "Collecting msgspec (from unsloth_zoo>=2025.9.6->unsloth)\n",
            "  Downloading msgspec-0.19.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: importlib_metadata in /usr/local/lib/python3.12/dist-packages (from diffusers->unsloth) (8.7.0)\n",
            "Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.12/dist-packages (from tyro->unsloth) (0.17.0)\n",
            "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.12/dist-packages (from tyro->unsloth) (13.9.4)\n",
            "Collecting shtab>=1.5.6 (from tyro->unsloth)\n",
            "  Downloading shtab-1.7.2-py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from tyro->unsloth) (4.4.4)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth) (3.12.15)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets<4.0.0,>=3.4.1->unsloth) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets<4.0.0,>=3.4.1->unsloth) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets<4.0.0,>=3.4.1->unsloth) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets<4.0.0,>=3.4.1->unsloth) (2025.8.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=11.1.0->tyro->unsloth) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=11.1.0->tyro->unsloth) (2.19.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.4.0->unsloth) (1.3.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib_metadata->diffusers->unsloth) (3.23.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.4.0->unsloth) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets<4.0.0,>=3.4.1->unsloth) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets<4.0.0,>=3.4.1->unsloth) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets<4.0.0,>=3.4.1->unsloth) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth) (1.20.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro->unsloth) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets<4.0.0,>=3.4.1->unsloth) (1.17.0)\n",
            "Downloading unsloth-2025.9.5-py3-none-any.whl (309 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.0/310.0 kB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trl-0.23.0-py3-none-any.whl (564 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m564.7/564.7 kB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bitsandbytes-0.47.0-py3-none-manylinux_2_24_x86_64.whl (61.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.3/61.3 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading unsloth_zoo-2025.9.6-py3-none-any.whl (208 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.3/208.3 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xformers-0.0.32.post2-cp39-abi3-manylinux_2_28_x86_64.whl (117.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.2/117.2 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tyro-0.9.31-py3-none-any.whl (131 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.7/131.7 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading shtab-1.7.2-py3-none-any.whl (14 kB)\n",
            "Downloading cut_cross_entropy-25.1.1-py3-none-any.whl (22 kB)\n",
            "Downloading msgspec-0.19.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.6/213.6 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: shtab, msgspec, tyro, xformers, datasets, cut_cross_entropy, bitsandbytes, trl, unsloth_zoo, unsloth\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 4.0.0\n",
            "    Uninstalling datasets-4.0.0:\n",
            "      Successfully uninstalled datasets-4.0.0\n",
            "Successfully installed bitsandbytes-0.47.0 cut_cross_entropy-25.1.1 datasets-3.6.0 msgspec-0.19.0 shtab-1.7.2 trl-0.23.0 tyro-0.9.31 unsloth-2025.9.5 unsloth_zoo-2025.9.6 xformers-0.0.32.post2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For GPU check\n",
        "import torch\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "print(f\"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iz7t8tLVBTdT",
        "outputId": "c7208f44-4eb7-42c5-84b7-c5a1b57a471f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA available: True\n",
            "GPU: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "import json\n",
        "\n",
        "# Define model parameters\n",
        "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "# Load the model and tokenizer from Hugging Face\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316,
          "referenced_widgets": [
            "365d0200e3c34df8be25ee24c1c38b31",
            "ac0cd713c6584ba6940596f28ba81552",
            "f2e9e9adfee94a599111d659bbcf724b",
            "213626f28ec7473da51b2ac309954ad1",
            "c57a6f88f92f4d1bbb2bd1a1b59c4645",
            "bec9b848b5f048e7a607e4819fc3ebe9",
            "7a7854cdda2646a89b5b6a6ef0ed1487",
            "2e210894dfdb4a43a18ad493acbb5c21",
            "b4ad82bc05a14db2b012e265115dda2c",
            "ac15a3571d8240a49aaa448a3ffc6236",
            "56ae158e846e4e8485c6909a46eeb7d0",
            "a15bc17e1d75487c966042272b41fb48",
            "832b7729882e4d09979d9498e1e8d78c",
            "406d1cfba7a444078dee2d56f8d935e2",
            "6965e3e0072b4d3c9f46f83f3b3ad2f1",
            "e6865022831d49728ef49196cafc1137",
            "6e77dac2f49b4d1ea4ee788105a86fd5",
            "1a7dd6f9dfa94fe7bde428030dc6bbac",
            "4d36e1a53cd344c28e5034e5c46330ee",
            "2c5811397b364d7e8c54a83c60d5667a",
            "da656a1737c041a98c799e0722269b81",
            "045ad37ba75541249bc04eab99d6c997",
            "17717f811b6a46f7a6fea265f81088ed",
            "73994fa586c045d6bf86f8197ff22f93",
            "8061057b064949e78a2f03fed851e8bf",
            "7026a5fed8924382949d7a687f0288c4",
            "8a5d680dbc3e4fd68f5817d7fd630c01",
            "13a33036e3894b2ead6ccaf9acd8858d",
            "847948463f27450a83bd097b6d9421b4",
            "0027ccb60e5945649da77dea4c81affb",
            "4bdc5434bc30483fbbc3f3827dbd9743",
            "86cb0d61a8274efca911381236ec3602",
            "690d2543a77f4a87a228cbf4fee6b462",
            "aeeeb97317d5479bb82ddcf6e5697d25",
            "1ff6c7044a904072b81902bedffdf357",
            "1d7a80d65a354001b2d807569d5d38e9",
            "37c16a9b8da041639ba4dd059edd242e",
            "0430f561904e4e43923d0c6a39e0b811",
            "8fd02d8d32ee427885be53a1ce2ef874",
            "a3170073d88749eea0bb6a6194424449",
            "2e26204d100a44a0b13edbd730726d5e",
            "835065177d55443f8ce11b6ff4c5b8ed",
            "3b908cae3872421a9ae6c31bf741799c",
            "f8b9bd13fa3a4a1a9795da37d76d2917",
            "25ef13c26f19433fa766a6773d7b617d",
            "fe100ba60d8543b4a272bb6eaa2ff4dc",
            "d73b7c0dd1eb4048838dbe374e6f3862",
            "f20b8111a7014f5a91d2e7e74966a639",
            "e01d4fcda2ac46b88d8981aca78cf66b",
            "15020033d493486c8baff8de47f96cdf",
            "4226e3714e1a4e3892e9a2c376a6c225",
            "69446aa220754f94a59b425f85b93899",
            "03bbca3f71e043d5ac8abbf93c48f201",
            "1a9a95f3eb7b4191a6be5f7204f597ba",
            "86aa122813e3457da85b8e7edae47f15"
          ]
        },
        "id": "vrvoOWAvWeFr",
        "outputId": "4af95c6c-d463-4a2b-ba26-a837e609506b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
            "==((====))==  Unsloth 2025.9.5: Fast Llama patching. Transformers: 4.56.1.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.8.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.4.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/5.70G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "365d0200e3c34df8be25ee24c1c38b31"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/220 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a15bc17e1d75487c966042272b41fb48"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "17717f811b6a46f7a6fea265f81088ed"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "aeeeb97317d5479bb82ddcf6e5697d25"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/345 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "25ef13c26f19433fa766a6773d7b617d"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "4JAd1CNuX3XG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filename = os.path.join(BASE_DIR, 'full_dataset.json')\n",
        "\n",
        "print(f\"\\nSuccessfully uploaded '{filename}'\")\n",
        "\n",
        "# Load the JSON data from the uploaded file\n",
        "with open(filename, 'r') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Quick check to see if data is loaded correctly\n",
        "print(\"Number of dialogues in the dataset:\", len(data.get('dialogues', [])))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ApYCjIPOWhrK",
        "outputId": "a0167a77-76a5-4158-bdbc-36a7921729c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Successfully uploaded '/content/full_dataset.json'\n",
            "Number of dialogues in the dataset: 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the formatting function\n",
        "def format_chat_data(data):\n",
        "    formatted_dialogues = []\n",
        "    # Llama-3 chat template from tokenizer\n",
        "    chat_template = tokenizer.apply_chat_template\n",
        "\n",
        "    # The JSON file contains a list of dialogues directly\n",
        "    dialogues_list = data # The root of your JSON is the list\n",
        "\n",
        "    for dialogue in dialogues_list:\n",
        "        # Iterate through turns, taking a user turn and the following assistant turn\n",
        "        for i in range(0, len(dialogue['turns']) - 1, 2):\n",
        "            if dialogue['turns'][i]['role'] == 'user' and dialogue['turns'][i+1]['role'] == 'assistant':\n",
        "                user_turn = dialogue['turns'][i]\n",
        "                assistant_turn = dialogue['turns'][i+1]\n",
        "\n",
        "                # Combine intervention steps into the assistant's text if they exist\n",
        "                assistant_text = assistant_turn['text']\n",
        "                if 'intervention_steps' in assistant_turn:\n",
        "                    steps_text = \"\\n\".join(assistant_turn['intervention_steps'])\n",
        "                    assistant_text += \"\\n\" + steps_text\n",
        "\n",
        "                # Create the message structure for the template\n",
        "                messages = [\n",
        "                    {\"role\": \"user\", \"content\": user_turn['text']},\n",
        "                    {\"role\": \"assistant\", \"content\": assistant_text}\n",
        "                ]\n",
        "\n",
        "                # Apply the template and add to our list\n",
        "                # We set add_generation_prompt=False because we provide the full conversation\n",
        "                formatted_text = chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
        "                formatted_dialogues.append({\"text\": formatted_text})\n",
        "\n",
        "    return formatted_dialogues\n",
        "\n",
        "# Process the data and load it into a Hugging Face Dataset\n",
        "formatted_data = format_chat_data(data['dialogues']) # Pass the list of dialogues\n",
        "from datasets import Dataset\n",
        "dataset = Dataset.from_list(formatted_data)\n",
        "\n",
        "# Let's check one formatted example\n",
        "print(\"\\nFormatted example:\\n\", dataset[0]['text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5gyBM8HYAyd",
        "outputId": "8a1dcf69-521e-46cb-f442-87a5949d1a38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Formatted example:\n",
            " <|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "I'm so overwhelmed with my project. I feel like I'm drowning and can't focus on anything.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "That sounds incredibly stressful. It's completely understandable to feel like that when you're under a lot of pressure. I'm here to listen if you want to talk more about it.<|eot_id|>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the formatting function\n",
        "def format_chat_data(data, tokenizer):\n",
        "    formatted_dialogues = []\n",
        "\n",
        "    # The JSON file contains a list of dialogues\n",
        "    dialogues_list = data\n",
        "\n",
        "    for dialogue in dialogues_list:\n",
        "        messages = []\n",
        "\n",
        "        for turn in dialogue['turns']:\n",
        "            # Create a user turn with emotion and intent labels\n",
        "            if turn['role'] == 'user':\n",
        "                formatted_text = f\"(emotion: {turn['emotion']}) (intent: {turn['intent']}) {turn['text']}\"\n",
        "                messages.append({\"role\": \"user\", \"content\": formatted_text})\n",
        "\n",
        "            # Create an assistant turn with emotion and intent labels\n",
        "            elif turn['role'] == 'assistant':\n",
        "                assistant_text = turn['text']\n",
        "\n",
        "                # Combine intervention steps if they exist\n",
        "                if 'intervention_steps' in turn:\n",
        "                    steps_text = \"\\n\".join(turn['intervention_steps'])\n",
        "                    assistant_text += \"\\n\" + steps_text\n",
        "\n",
        "                # Add the emotion and intent labels to the assistant's text\n",
        "                formatted_text = f\"(emotion: {turn['emotion']}) (intent: {turn['intent']}) {assistant_text}\"\n",
        "                messages.append({\"role\": \"assistant\", \"content\": formatted_text})\n",
        "\n",
        "        # Apply the template to the full conversation and add to our list\n",
        "        # We set add_generation_prompt=False because we provide the full conversation\n",
        "        formatted_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
        "        formatted_dialogues.append({\"text\": formatted_text})\n",
        "\n",
        "    return formatted_dialogues\n",
        "\n",
        "# Example of how to use it\n",
        "# You would load your data and tokenizer first\n",
        "formatted_data = format_chat_data(data['dialogues'], tokenizer)\n",
        "\n",
        "from datasets import Dataset\n",
        "dataset = Dataset.from_list(formatted_data)\n",
        "\n",
        "# Let's check one formatted example\n",
        "print(\"\\nFormatted example:\\n\", dataset[0]['text'])"
      ],
      "metadata": {
        "id": "5mDyWXr0pdDX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8fda8a5-2b84-4d31-ea6a-9e7225afdec1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Formatted example:\n",
            " <|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "(emotion: stress) (intent: venting_intent) I'm so overwhelmed with my project. I feel like I'm drowning and can't focus on anything.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "(emotion: calmness) (intent: general_chat_intent) That sounds incredibly stressful. It's completely understandable to feel like that when you're under a lot of pressure. I'm here to listen if you want to talk more about it.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "(emotion: frustration) (intent: continue_venting_intent) Yeah, and my boss just keeps adding more tasks. I just don't know where to start or how I'm going to get it all done.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "(emotion: calmness) (intent: general_chat_intent) It sounds really frustrating when you have so much on your plate. Can you tell me more about what's making it feel so difficult right now?<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "(emotion: frustration) (intent: continue_venting_intent) I've been working on this for three weeks straight and I'm just so exhausted. I don't feel like I'll ever finish.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "(emotion: calmness) (intent: general_chat_intent) That sounds incredibly draining. It's tough when you feel like the finish line is still so far away.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "(emotion: stress) (intent: seeking_solution_intent) Yeah... I feel like I'm having a panic attack just thinking about it. I need a way to stop this feeling and calm down. Can you help me?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "(emotion: calmness) (intent: general_chat_intent) It sounds like you're going through a lot right now, and I want to help you feel grounded. I can guide you through a deep-breathing exercise to help you feel calmer. We can use the 4-7-8 breathing technique. Here are the steps:\n",
            "1. Breathe in deeply through your nose for a count of four.\n",
            "2. Hold that breath for a count of seven.\n",
            "3. Slowly release your breath through your mouth for a count of eight.<|eot_id|>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(dataset)"
      ],
      "metadata": {
        "id": "jCx4EQ-hYFyq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4f72798-4830-435a-8f85-3e90d13076a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The new, more flexible formatting function\n",
        "def format_chat_data(data, tokenizer):\n",
        "    formatted_dialogues = []\n",
        "    dialogues_list = data\n",
        "\n",
        "    for dialogue in dialogues_list:\n",
        "        messages = []\n",
        "        for turn in dialogue['turns']:\n",
        "            # Start with an empty prefix for our special tags\n",
        "            prefix = \"\"\n",
        "\n",
        "            # Conditionally add emotion if it exists\n",
        "            if 'emotion' in turn:\n",
        "                prefix += f\"(emotion: {turn['emotion']}) \"\n",
        "\n",
        "            # Conditionally add intent if it exists\n",
        "            if 'intent' in turn:\n",
        "                prefix += f\"(intent: {turn['intent']}) \"\n",
        "\n",
        "            # Get the main text of the turn\n",
        "            main_text = turn['text']\n",
        "\n",
        "            # For assistant turns, append intervention steps if they exist\n",
        "            if turn['role'] == 'assistant' and 'intervention_steps' in turn:\n",
        "                steps_text = \"\\n\".join(turn['intervention_steps'])\n",
        "                main_text += \"\\n\" + steps_text\n",
        "\n",
        "            # Combine the prefix and the main text\n",
        "            formatted_content = f\"{prefix}{main_text}\".strip()\n",
        "\n",
        "            messages.append({\"role\": turn['role'], \"content\": formatted_content})\n",
        "\n",
        "        # Apply the chat template to the entire conversation\n",
        "        formatted_text = tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=False\n",
        "        )\n",
        "        formatted_dialogues.append({\"text\": formatted_text})\n",
        "\n",
        "    return formatted_dialogues"
      ],
      "metadata": {
        "id": "-z7_3zP-fcN3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from datasets import Dataset\n",
        "\n",
        "# Step 1: Load both datasets\n",
        "with open('full_dataset.json', 'r') as f:\n",
        "    well_being_data = json.load(f)\n",
        "with open('personal_info_dataset.json', 'r') as f:\n",
        "    persona_data = json.load(f)\n",
        "\n",
        "well_being_data = well_being_data\n",
        "persona_data = persona_data\n",
        "\n",
        "\n",
        "# Step 2: Combine the dialogues from both files into one list\n",
        "all_dialogues = well_being_data['dialogues'] + persona_data['dialogues']\n",
        "print(f\"Total dialogues to be trained on: {len(all_dialogues)}\")\n",
        "\n",
        "\n",
        "# Step 3: Process the combined list with our new, robust function\n",
        "# (Make sure to pass your actual tokenizer object here)\n",
        "formatted_data = format_chat_data(all_dialogues, tokenizer)\n",
        "final_dataset = Dataset.from_list(formatted_data)\n",
        "\n",
        "\n",
        "# --- Verification ---\n",
        "# Let's check that both types of data were formatted correctly\n",
        "\n",
        "print(\"\\n--- Formatted Well-being Example ---\")\n",
        "# The well-being example should have both emotion and intent tags\n",
        "print(final_dataset[0]['text'])\n",
        "\n",
        "print(\"\\n--- Formatted Persona Example ---\")\n",
        "# The persona example should only have the intent tag for the user\n",
        "# and no tags for the assistant\n",
        "print(final_dataset[101]['text'])"
      ],
      "metadata": {
        "id": "Yd02VwmlgAEc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47d0d8c5-0495-451d-d989-4bed4d8962aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total dialogues to be trained on: 115\n",
            "\n",
            "--- Formatted Well-being Example ---\n",
            "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "(emotion: stress) (intent: venting_intent) I'm so overwhelmed with my project. I feel like I'm drowning and can't focus on anything.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "(emotion: calmness) (intent: general_chat_intent) That sounds incredibly stressful. It's completely understandable to feel like that when you're under a lot of pressure. I'm here to listen if you want to talk more about it.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "(emotion: frustration) (intent: continue_venting_intent) Yeah, and my boss just keeps adding more tasks. I just don't know where to start or how I'm going to get it all done.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "(emotion: calmness) (intent: general_chat_intent) It sounds really frustrating when you have so much on your plate. Can you tell me more about what's making it feel so difficult right now?<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "(emotion: frustration) (intent: continue_venting_intent) I've been working on this for three weeks straight and I'm just so exhausted. I don't feel like I'll ever finish.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "(emotion: calmness) (intent: general_chat_intent) That sounds incredibly draining. It's tough when you feel like the finish line is still so far away.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "(emotion: stress) (intent: seeking_solution_intent) Yeah... I feel like I'm having a panic attack just thinking about it. I need a way to stop this feeling and calm down. Can you help me?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "(emotion: calmness) (intent: general_chat_intent) It sounds like you're going through a lot right now, and I want to help you feel grounded. I can guide you through a deep-breathing exercise to help you feel calmer. We can use the 4-7-8 breathing technique. Here are the steps:\n",
            "1. Breathe in deeply through your nose for a count of four.\n",
            "2. Hold that breath for a count of seven.\n",
            "3. Slowly release your breath through your mouth for a count of eight.<|eot_id|>\n",
            "\n",
            "--- Formatted Persona Example ---\n",
            "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "(intent: general_chat_intent) Who made you?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "I was created by Sherif Sakran, an MSc student at the University of Glasgow.<|eot_id|>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add LoRA adapters to the model\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16, # Choose any number > 0. Suggested values are 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bBv93kmYDzm",
        "outputId": "880463a7-0d52-4b07-bfb7-0fce05feaabd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth 2025.9.5 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "# Configure and run the trainer\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False,\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        max_steps = 60,\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not torch.cuda.is_bf16_supported(),\n",
        "        bf16 = torch.cuda.is_bf16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"none\", # This is the line to add/change!**\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "45c60d0a59fe4fe2ae04f88878e8694c",
            "3fc4a5eeb9924827b226dacb9eab84ae",
            "a500b3179a174c14aa61bb577d7a5d33",
            "c9d17ebabd08481390785d7d3e11ed9f",
            "b8a197f2655c4c5f8728fc3dc02d9e36",
            "0b632ae327bc40269900ad2b6a2ea9b3",
            "f2043d9ca26f4633be1d5b734325d699",
            "f8e0ea4aaa8f4aabb9e59e1d207b9225",
            "450d55d5227446dfbf6bc4d9b4831ea3",
            "01457646a85c4b51a9b9452e4bbd56e1",
            "0a01bcbec6fb48cd902eb485116c2b20"
          ]
        },
        "id": "gF1A3YAcYxuK",
        "outputId": "e105337c-9c95-46e5-bb23-be47054e0a6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Unsloth: Tokenizing [\"text\"] (num_proc=6):   0%|          | 0/100 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "45c60d0a59fe4fe2ae04f88878e8694c"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Start training!\n",
        "trainer_stats = trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "JcVWDa79YzDz",
        "outputId": "24a9bd2f-7486-4d77-b1a3-59fa099f9d5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 100 | Num Epochs = 5 | Total steps = 60\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
            " \"-____-\"     Trainable parameters = 41,943,040 of 8,072,204,288 (0.52% trained)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Will smartly offload gradients to save VRAM!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [60/60 08:43, Epoch 4/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.061900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.152300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.044800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.954400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.678400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.413200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>1.261200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.138100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>1.043300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.921000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.818100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.792900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.776700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.751300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.649200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.628400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.590900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.544700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.567800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.559700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>0.480800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>0.483100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>0.459900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>0.499900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.470400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>0.393800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>0.408400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>0.369400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>0.377500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.442100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>0.389500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>0.341600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>0.330100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>0.290300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>0.288400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>0.298900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>0.241200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>0.312500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>0.262900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.275900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>0.292600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>0.235600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>0.243000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>0.200200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>0.215300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>0.297300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>0.185100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>0.234600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>0.195300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.191100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>0.197900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>0.187100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53</td>\n",
              "      <td>0.176200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>0.193000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>0.182300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>0.181900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>0.199400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58</td>\n",
              "      <td>0.181200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59</td>\n",
              "      <td>0.170700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.158500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an inference prompt\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"I feel really overwhelmed and alone. It feels like nobody understands.\"}\n",
        "]\n",
        "\n",
        "# Unsloth's fast inference pipeline\n",
        "# We must use the chat template!\n",
        "input_ids = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(input_ids=input_ids, max_new_tokens=128, use_cache=True)\n",
        "response = tokenizer.batch_decode(outputs)\n",
        "\n",
        "print(\"Fine-tuned model's response:\\n\", response[0].split(\"<|end_header_id|>\\n\\n\")[2].replace(\"<|eot_id|>\", \"\").strip())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5whHunHaY2yT",
        "outputId": "06624068-e976-4e08-8b7a-faf0db9df37e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fine-tuned model's response:\n",
            " It's completely valid to feel this way. It sounds like you're carrying a heavy weight and it's okay to feel that way. I'm here to listen if you want to talk more about it.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure llama-cpp-python is installed for GGUF conversion\n",
        "# !pip install llama-cpp-python"
      ],
      "metadata": {
        "id": "KjNroS9tbCRC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Manually compile llama.cpp\n",
        "# # This might take a few minutes\n",
        "# !git clone --recursive https://github.com/ggerganov/llama.cpp\n",
        "# %cd llama.cpp\n",
        "# !make clean && make all -j\n",
        "# %cd /content/"
      ],
      "metadata": {
        "id": "9EJaaGgbvpIg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fc8031a8"
      },
      "source": [
        "# # Manually compile llama.cpp using CMake\n",
        "# # This might take a few minutes\n",
        "# !git clone --recursive https://github.com/ggerganov/llama.cpp\n",
        "# %cd llama.cpp\n",
        "# !mkdir build\n",
        "# %cd build\n",
        "# !cmake ..\n",
        "# !cmake --build . --config Release -j\n",
        "# %cd /content/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge the LoRA adapters and save to GGUF\n",
        "# You can choose different quantization methods. q4_k_m is a great balance of size and performance.\n",
        "# model.save_pretrained_gguf(\"llama3-8b-sakinah2-gguf\", tokenizer, quantization_method = \"q4_k_m\")\n",
        "\n",
        "# This will create a file like:\n",
        "# llama3-8b-therapist-gguf/ggml-model-q4_k_m.gguf"
      ],
      "metadata": {
        "id": "dmsDx26VYDpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instead of save_pretrained_gguf, use this:\n",
        "# This merges the adapters and saves the full model in 16-bit precision.\n",
        "merged_model_path = \"llama3_therapist_merged\"\n",
        "model.save_pretrained_merged(merged_model_path, tokenizer, save_method = \"merged_16bit\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432,
          "referenced_widgets": [
            "ceb080df29494f64b78358b411b0e987",
            "1eaec69766954bb69a6298a891359f5c",
            "70dd6c27ab1c4d979a85918c7dcd803c",
            "5079dd9bb4694e448ededa2304920593",
            "5fa7f9cd91664222bbf6df0b1f6d5ee0",
            "505a4fd824164ebbb6ed52f276208c72",
            "293e068e9c4e444192036bd72ab93904",
            "17cd86844f9c4ae199c52dd205e8e539",
            "cd5e364eed5743a199ebb9dab1f109d9",
            "aef3bca00d3342848300f7a7025c3cbb",
            "c4619ba7d7b349c3ae5870bd3c211e71",
            "7bc0027616104100a85f46bb232241c8",
            "39a4c73635b44e03932518d620b65379",
            "74f16b1fe19940dea6d7b01810ef97df",
            "c52e48a67eca4e5697000d509fa5e3a4",
            "059efc601d6c41a993bd68c1e2eb3f7d",
            "979944fa35e94d7a8429c1420ace9470",
            "b2f7c5da27044983a607bbad6acf0735",
            "7ce6d3f0e53c4fdf92b62532377797f0",
            "ca15d9b413764b7985352456939ff0be",
            "5366bd6f22284095b9368231ebf23c2a",
            "6b44173aef4f4a379ba0fe5d0ee57146",
            "1a611f3c86324e99a3e1afd56c1d98d6",
            "f88a46db78564ff3bbeaad713adf10df",
            "11b18478cc2042fa82801c945cbe4b4b",
            "64656fef6e77449baa880b04102d7311",
            "9e22a4c95db048b89db39613378a090a",
            "a526812054d94197a272841c7e78a372",
            "73fdc5b22a8e4697b2c260d503a28da0",
            "003be1d3c92e4f589b9328928e8d8ebd",
            "1f3a45a86dd246c492a02f3acba035e3",
            "8a377cba540940edb46921728abbc0a4",
            "9a5eaea9ad9c494b9b279d5402c2f946",
            "439789527c0c45629710f1731c7f1ebd",
            "9ed6db4a1f654ed085d33b42cc7058a2",
            "98e86d708c984395b7664ced757348cd",
            "1bdb457cdbef439c880b6fea28ab3230",
            "43a0c2186f194905a429d00e52f3d0b9",
            "ed259152ea914861895fc23d29c65c6f",
            "4c39fc5939304431b14b9167c66ce410",
            "bed510c8f71347bfb29c4787441bd9e6",
            "8253b83d76cb466cb1ccd4ef14c9315c",
            "28770ec4eb984adfb948f805c18d1396",
            "c0c2a007e1bc477398a4a46ce4c10376",
            "ef16d4263b0a4dd3ae280ff424d2d64e",
            "ce585d84e1554975a13f5f8cb8378ece",
            "16c3e58579f34f40a3b50d3349755814",
            "4ae4f535f58f44db9345cc324d5d317a",
            "f469a6f38bed41aead079a98421eb953",
            "2393cc37c1e64c9394831cc528563f0e",
            "7c745ee261e243988a25f2c607bb61db",
            "1a5a1beb33254a4abb9800a4f4e1954b",
            "dc0d12133138440b9ebd03d9e8222fef",
            "fef1447258d644da9c4cc45ea77e54c2",
            "8a152f3b4d3c47df9624b0eb2bcef51e",
            "482b72e3789c444d8ae5d46e3ae64970",
            "e929b8322d64418a84c5b350744c9580",
            "1cedb43ac49b40c7adf52bd2647157f9",
            "5e12bbaed3254d4190813ff10ac7bae3",
            "d39c145a68114ed18dd0519d9effe522",
            "3094af5b394a4c0d84ff9baeb1526ff6",
            "4e420d1690c245f5a062e67573dd0070",
            "855cecd94aca4d0e892c223de365ea1d",
            "78884c5d2a2c40bc9f8b91518c0d07c8",
            "77325456fab342b8a6c69e483ee03527",
            "933d75eea9dc4c6bb7fd4a9102fba5aa",
            "305aadf37840455797e993d4efb1b689",
            "e0f40d075fe242898ac09f1c36b5ebcb",
            "cb05b30a5bff406e8e2a2a960a26cd76",
            "fb654907f1834a1588ef43d2478fe30d",
            "1f055d8db9004a96a8e795ffd1afde66",
            "9c816983d3e043aba7e7b7fb19a7386d",
            "5cac21250d484228be5500703869441a",
            "37128cfe7a564aa487dfb1c3539c5ae2",
            "cdbe58da68554ce5bb37fb77505346df",
            "2091645759784cb78845a9b5cefcad58",
            "11f63e3d83054b9282011e04fe6500df"
          ]
        },
        "id": "1p-XE9qXbhUD",
        "outputId": "d83668d9-680f-4d53-f1fd-5dddbf030e2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/777 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ceb080df29494f64b78358b411b0e987"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG: Removed incorrectly saved weight file: model-00002-of-00002.safetensors\n",
            "DEBUG: Removed incorrectly saved weight file: model-00001-of-00002.safetensors\n",
            "Found HuggingFace hub cache directory: /root/.cache/huggingface/hub\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7bc0027616104100a85f46bb232241c8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1a611f3c86324e99a3e1afd56c1d98d6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking cache directory for required files...\n",
            "Cache check failed: model-00001-of-00004.safetensors not found in local cache.\n",
            "Not all required files found in cache. Will proceed with downloading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rUnsloth: Merging weights into 16bit:   0%|          | 0/4 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "439789527c0c45629710f1731c7f1ebd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rUnsloth: Merging weights into 16bit:  25%|██▌       | 1/4 [32:57<1:38:53, 1977.87s/it]"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ef16d4263b0a4dd3ae280ff424d2d64e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rUnsloth: Merging weights into 16bit:  50%|█████     | 2/4 [46:56<43:35, 1307.99s/it]  "
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "482b72e3789c444d8ae5d46e3ae64970"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rUnsloth: Merging weights into 16bit:  75%|███████▌  | 3/4 [1:01:31<18:30, 1110.16s/it]"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "305aadf37840455797e993d4efb1b689"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth: Merging weights into 16bit: 100%|██████████| 4/4 [1:04:34<00:00, 968.64s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Converting the model into gguf"
      ],
      "metadata": {
        "id": "-_9cwqaqWJum"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean up any old attempts\n",
        "# !rm -rf llama.cpp\n",
        "\n",
        "# Clone the repository (using --depth 1 for a clean, lightweight clone)\n",
        "print(\"⬇️ Cloning llama.cpp repository...\")\n",
        "!git clone --depth 1 https://github.com/ggerganov/llama.cpp\n",
        "\n",
        "# List ALL files recursively to find the converter\n",
        "print(\"\\n✅ Clone complete. Now searching for the conversion script...\")\n",
        "print(\"----------------------------------------------------------------\")\n",
        "print(\"Look through the list below for a file named 'convert-hf-to-gguf.py' or 'convert.py'.\")\n",
        "print(\"Copy its FULL PATH (e.g., /content/llama.cpp/convert-hf-to-gguf.py)\")\n",
        "print(\"----------------------------------------------------------------\\n\")\n",
        "\n",
        "!ls -R /content/llama.cpp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-f8kTLrI9RP3",
        "outputId": "14fd98c0-37e5-48a4-b792-efb1397303cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⬇️ Cloning llama.cpp repository...\n",
            "Cloning into 'llama.cpp'...\n",
            "remote: Enumerating objects: 1652, done.\u001b[K\n",
            "remote: Counting objects: 100% (1652/1652), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1281/1281), done.\u001b[K\n",
            "remote: Total 1652 (delta 346), reused 1067 (delta 315), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (1652/1652), 24.18 MiB | 20.18 MiB/s, done.\n",
            "Resolving deltas: 100% (346/346), done.\n",
            "\n",
            "✅ Clone complete. Now searching for the conversion script...\n",
            "----------------------------------------------------------------\n",
            "Look through the list below for a file named 'convert-hf-to-gguf.py' or 'convert.py'.\n",
            "Copy its FULL PATH (e.g., /content/llama.cpp/convert-hf-to-gguf.py)\n",
            "----------------------------------------------------------------\n",
            "\n",
            "/content/llama.cpp:\n",
            "AUTHORS\t\t\t       examples    poetry.lock\n",
            "build-xcframework.sh\t       flake.lock  prompts\n",
            "ci\t\t\t       flake.nix   pyproject.toml\n",
            "cmake\t\t\t       ggml\t   pyrightconfig.json\n",
            "CMakeLists.txt\t\t       gguf-py\t   README.md\n",
            "CMakePresets.json\t       grammars    requirements\n",
            "CODEOWNERS\t\t       include\t   requirements.txt\n",
            "common\t\t\t       LICENSE\t   scripts\n",
            "CONTRIBUTING.md\t\t       licenses    SECURITY.md\n",
            "convert_hf_to_gguf.py\t       Makefile    src\n",
            "convert_hf_to_gguf_update.py   media\t   tests\n",
            "convert_llama_ggml_to_gguf.py  models\t   tools\n",
            "convert_lora_to_gguf.py        mypy.ini    vendor\n",
            "docs\t\t\t       pocs\n",
            "\n",
            "/content/llama.cpp/ci:\n",
            "README.md  run.sh\n",
            "\n",
            "/content/llama.cpp/cmake:\n",
            "arm64-apple-clang.cmake   common.cmake\t\t llama.pc.in\n",
            "arm64-windows-llvm.cmake  git-vars.cmake\t x64-windows-llvm.cmake\n",
            "build-info.cmake\t  llama-config.cmake.in\n",
            "\n",
            "/content/llama.cpp/common:\n",
            "arg.cpp\t\t   common.h\t\t       ngram-cache.cpp\n",
            "arg.h\t\t   console.cpp\t\t       ngram-cache.h\n",
            "base64.hpp\t   console.h\t\t       regex-partial.cpp\n",
            "build-info.cpp.in  json-partial.cpp\t       regex-partial.h\n",
            "chat.cpp\t   json-partial.h\t       sampling.cpp\n",
            "chat.h\t\t   json-schema-to-grammar.cpp  sampling.h\n",
            "chat-parser.cpp    json-schema-to-grammar.h    speculative.cpp\n",
            "chat-parser.h\t   llguidance.cpp\t       speculative.h\n",
            "CMakeLists.txt\t   log.cpp\n",
            "common.cpp\t   log.h\n",
            "\n",
            "/content/llama.cpp/docs:\n",
            "android.md  build-s390x.md  function-calling.md  multimodal\tops.md\n",
            "backend     development     install.md\t\t multimodal.md\n",
            "build.md    docker.md\t    llguidance.md\t ops\n",
            "\n",
            "/content/llama.cpp/docs/backend:\n",
            "BLIS.md  CANN.md  CUDA-FEDORA.md  OPENCL.md  SYCL.md\n",
            "\n",
            "/content/llama.cpp/docs/development:\n",
            "debugging-tests.md  llama-star\n",
            "HOWTO-add-model.md  token_generation_performance_tips.md\n",
            "\n",
            "/content/llama.cpp/docs/development/llama-star:\n",
            "idea-arch.key  idea-arch.pdf\n",
            "\n",
            "/content/llama.cpp/docs/multimodal:\n",
            "gemma3.md\t  llava.md\t  minicpmv2.5.md  minicpmv4.5.md\n",
            "glmedge.md\t  minicpmo2.6.md  minicpmv2.6.md  MobileVLM.md\n",
            "granitevision.md  minicpmo4.0.md  minicpmv4.0.md\n",
            "\n",
            "/content/llama.cpp/docs/ops:\n",
            "BLAS.csv  CPU.csv   Metal.csv\tSYCL.csv    zDNN.csv\n",
            "CANN.csv  CUDA.csv  OpenCL.csv\tVulkan.csv\n",
            "\n",
            "/content/llama.cpp/examples:\n",
            "batched\t\t\t\t llm.vim\n",
            "batched.swift\t\t\t lookahead\n",
            "chat-13B.bat\t\t\t lookup\n",
            "chat-13B.sh\t\t\t Miku.sh\n",
            "chat-persistent.sh\t\t model-conversion\n",
            "chat.sh\t\t\t\t parallel\n",
            "chat-vicuna.sh\t\t\t passkey\n",
            "CMakeLists.txt\t\t\t pydantic_models_to_grammar_examples.py\n",
            "convert_legacy_llama.py\t\t pydantic_models_to_grammar.py\n",
            "convert-llama2c-to-ggml\t\t reason-act.sh\n",
            "deprecation-warning\t\t regex_to_grammar.py\n",
            "diffusion\t\t\t retrieval\n",
            "embedding\t\t\t save-load-state\n",
            "eval-callback\t\t\t server_embd.py\n",
            "gen-docs\t\t\t server-llama2-13B.sh\n",
            "gguf\t\t\t\t simple\n",
            "gguf-hash\t\t\t simple-chat\n",
            "gritlm\t\t\t\t simple-cmake-pkg\n",
            "jeopardy\t\t\t speculative\n",
            "json_schema_pydantic_example.py  speculative-simple\n",
            "json_schema_to_grammar.py\t sycl\n",
            "llama.android\t\t\t training\n",
            "llama.swiftui\t\t\t ts-type-to-grammar.sh\n",
            "llama.vim\n",
            "\n",
            "/content/llama.cpp/examples/batched:\n",
            "batched.cpp  CMakeLists.txt  README.md\n",
            "\n",
            "/content/llama.cpp/examples/batched.swift:\n",
            "Makefile  Package.swift  README.md  Sources\n",
            "\n",
            "/content/llama.cpp/examples/batched.swift/Sources:\n",
            "main.swift\n",
            "\n",
            "/content/llama.cpp/examples/convert-llama2c-to-ggml:\n",
            "CMakeLists.txt\tconvert-llama2c-to-ggml.cpp  README.md\n",
            "\n",
            "/content/llama.cpp/examples/deprecation-warning:\n",
            "deprecation-warning.cpp  README.md\n",
            "\n",
            "/content/llama.cpp/examples/diffusion:\n",
            "CMakeLists.txt\tdiffusion-cli.cpp  README.md\n",
            "\n",
            "/content/llama.cpp/examples/embedding:\n",
            "CMakeLists.txt\tembedding.cpp  README.md\n",
            "\n",
            "/content/llama.cpp/examples/eval-callback:\n",
            "CMakeLists.txt\teval-callback.cpp  README.md\n",
            "\n",
            "/content/llama.cpp/examples/gen-docs:\n",
            "CMakeLists.txt\tgen-docs.cpp\n",
            "\n",
            "/content/llama.cpp/examples/gguf:\n",
            "CMakeLists.txt\tgguf.cpp\n",
            "\n",
            "/content/llama.cpp/examples/gguf-hash:\n",
            "CMakeLists.txt\tdeps  gguf-hash.cpp  README.md\n",
            "\n",
            "/content/llama.cpp/examples/gguf-hash/deps:\n",
            "rotate-bits  sha1  sha256  xxhash\n",
            "\n",
            "/content/llama.cpp/examples/gguf-hash/deps/rotate-bits:\n",
            "package.json  rotate-bits.h\n",
            "\n",
            "/content/llama.cpp/examples/gguf-hash/deps/sha1:\n",
            "package.json  sha1.c  sha1.h\n",
            "\n",
            "/content/llama.cpp/examples/gguf-hash/deps/sha256:\n",
            "package.json  sha256.c\tsha256.h\n",
            "\n",
            "/content/llama.cpp/examples/gguf-hash/deps/xxhash:\n",
            "clib.json  xxhash.c  xxhash.h\n",
            "\n",
            "/content/llama.cpp/examples/gritlm:\n",
            "CMakeLists.txt\tgritlm.cpp  README.md\n",
            "\n",
            "/content/llama.cpp/examples/jeopardy:\n",
            "graph.py  jeopardy.sh  qasheet.csv  questions.txt  README.md\n",
            "\n",
            "/content/llama.cpp/examples/llama.android:\n",
            "app\t\t  gradle\t     gradlew  README.md\n",
            "build.gradle.kts  gradle.properties  llama    settings.gradle.kts\n",
            "\n",
            "/content/llama.cpp/examples/llama.android/app:\n",
            "build.gradle.kts  proguard-rules.pro  src\n",
            "\n",
            "/content/llama.cpp/examples/llama.android/app/src:\n",
            "main\n",
            "\n",
            "/content/llama.cpp/examples/llama.android/app/src/main:\n",
            "AndroidManifest.xml  java  res\n",
            "\n",
            "/content/llama.cpp/examples/llama.android/app/src/main/java:\n",
            "com\n",
            "\n",
            "/content/llama.cpp/examples/llama.android/app/src/main/java/com:\n",
            "example\n",
            "\n",
            "/content/llama.cpp/examples/llama.android/app/src/main/java/com/example:\n",
            "llama\n",
            "\n",
            "/content/llama.cpp/examples/llama.android/app/src/main/java/com/example/llama:\n",
            "Downloadable.kt  MainActivity.kt  MainViewModel.kt  ui\n",
            "\n",
            "/content/llama.cpp/examples/llama.android/app/src/main/java/com/example/llama/ui:\n",
            "theme\n",
            "\n",
            "/content/llama.cpp/examples/llama.android/app/src/main/java/com/example/llama/ui/theme:\n",
            "Color.kt  Theme.kt  Type.kt\n",
            "\n",
            "/content/llama.cpp/examples/llama.android/app/src/main/res:\n",
            "drawable       mipmap-hdpi  mipmap-xhdpi   mipmap-xxxhdpi  xml\n",
            "mipmap-anydpi  mipmap-mdpi  mipmap-xxhdpi  values\n",
            "\n",
            "/content/llama.cpp/examples/llama.android/app/src/main/res/drawable:\n",
            "ic_launcher_background.xml  ic_launcher_foreground.xml\n",
            "\n",
            "/content/llama.cpp/examples/llama.android/app/src/main/res/mipmap-anydpi:\n",
            "ic_launcher_round.xml  ic_launcher.xml\n",
            "\n",
            "/content/llama.cpp/examples/llama.android/app/src/main/res/mipmap-hdpi:\n",
            "ic_launcher_round.webp\tic_launcher.webp\n",
            "\n",
            "/content/llama.cpp/examples/llama.android/app/src/main/res/mipmap-mdpi:\n",
            "ic_launcher_round.webp\tic_launcher.webp\n",
            "\n",
            "/content/llama.cpp/examples/llama.android/app/src/main/res/mipmap-xhdpi:\n",
            "ic_launcher_round.webp\tic_launcher.webp\n",
            "\n",
            "/content/llama.cpp/examples/llama.android/app/src/main/res/mipmap-xxhdpi:\n",
            "ic_launcher_round.webp\tic_launcher.webp\n",
            "\n",
            "/content/llama.cpp/examples/llama.android/app/src/main/res/mipmap-xxxhdpi:\n",
            "ic_launcher_round.webp\tic_launcher.webp\n",
            "\n",
            "/content/llama.cpp/examples/llama.android/app/src/main/res/values:\n",
            "colors.xml  strings.xml  themes.xml\n",
            "\n",
            "/content/llama.cpp/examples/llama.android/app/src/main/res/xml:\n",
            "backup_rules.xml  data_extraction_rules.xml\n",
            "\n",
            "/content/llama.cpp/examples/llama.android/gradle:\n",
            "wrapper\n",
            "\n",
            "/content/llama.cpp/examples/llama.android/gradle/wrapper:\n",
            "gradle-wrapper.jar  gradle-wrapper.properties\n",
            "\n",
            "/content/llama.cpp/examples/llama.android/llama:\n",
            "build.gradle.kts  consumer-rules.pro  proguard-rules.pro  src\n",
            "\n",
            "/content/llama.cpp/examples/llama.android/llama/src:\n",
            "androidTest  main  test\n",
            "\n",
            "/content/llama.cpp/examples/llama.android/llama/src/androidTest:\n",
            "java\n",
            "\n",
            "/content/llama.cpp/examples/llama.android/llama/src/androidTest/java:\n",
            "android\n",
            "\n",
            "/content/llama.cpp/examples/llama.android/llama/src/androidTest/java/android:\n",
            "llama\n",
            "\n",
            "/content/llama.cpp/examples/llama.android/llama/src/androidTest/java/android/llama:\n",
            "cpp\n",
            "\n",
            "/content/llama.cpp/examples/llama.android/llama/src/androidTest/java/android/llama/cpp:\n",
            "ExampleInstrumentedTest.kt\n",
            "\n",
            "/content/llama.cpp/examples/llama.android/llama/src/main:\n",
            "AndroidManifest.xml  cpp  java\n",
            "\n",
            "/content/llama.cpp/examples/llama.android/llama/src/main/cpp:\n",
            "CMakeLists.txt\tllama-android.cpp\n",
            "\n",
            "/content/llama.cpp/examples/llama.android/llama/src/main/java:\n",
            "android\n",
            "\n",
            "/content/llama.cpp/examples/llama.android/llama/src/main/java/android:\n",
            "llama\n",
            "\n",
            "/content/llama.cpp/examples/llama.android/llama/src/main/java/android/llama:\n",
            "cpp\n",
            "\n",
            "/content/llama.cpp/examples/llama.android/llama/src/main/java/android/llama/cpp:\n",
            "LLamaAndroid.kt\n",
            "\n",
            "/content/llama.cpp/examples/llama.android/llama/src/test:\n",
            "java\n",
            "\n",
            "/content/llama.cpp/examples/llama.android/llama/src/test/java:\n",
            "android\n",
            "\n",
            "/content/llama.cpp/examples/llama.android/llama/src/test/java/android:\n",
            "llama\n",
            "\n",
            "/content/llama.cpp/examples/llama.android/llama/src/test/java/android/llama:\n",
            "cpp\n",
            "\n",
            "/content/llama.cpp/examples/llama.android/llama/src/test/java/android/llama/cpp:\n",
            "ExampleUnitTest.kt\n",
            "\n",
            "/content/llama.cpp/examples/llama.swiftui:\n",
            "llama.cpp.swift  llama.swiftui\tllama.swiftui.xcodeproj  README.md\n",
            "\n",
            "/content/llama.cpp/examples/llama.swiftui/llama.cpp.swift:\n",
            "LibLlama.swift\n",
            "\n",
            "/content/llama.cpp/examples/llama.swiftui/llama.swiftui:\n",
            "Assets.xcassets  llama_swiftuiApp.swift  Models  Resources  UI\n",
            "\n",
            "/content/llama.cpp/examples/llama.swiftui/llama.swiftui/Assets.xcassets:\n",
            "AppIcon.appiconset  Contents.json\n",
            "\n",
            "/content/llama.cpp/examples/llama.swiftui/llama.swiftui/Assets.xcassets/AppIcon.appiconset:\n",
            "Contents.json\n",
            "\n",
            "/content/llama.cpp/examples/llama.swiftui/llama.swiftui/Models:\n",
            "LlamaState.swift\n",
            "\n",
            "/content/llama.cpp/examples/llama.swiftui/llama.swiftui/Resources:\n",
            "models\n",
            "\n",
            "/content/llama.cpp/examples/llama.swiftui/llama.swiftui/Resources/models:\n",
            "\n",
            "/content/llama.cpp/examples/llama.swiftui/llama.swiftui/UI:\n",
            "ContentView.swift     InputButton.swift\n",
            "DownloadButton.swift  LoadCustomButton.swift\n",
            "\n",
            "/content/llama.cpp/examples/llama.swiftui/llama.swiftui.xcodeproj:\n",
            "project.pbxproj  project.xcworkspace\n",
            "\n",
            "/content/llama.cpp/examples/llama.swiftui/llama.swiftui.xcodeproj/project.xcworkspace:\n",
            "contents.xcworkspacedata  xcshareddata\n",
            "\n",
            "/content/llama.cpp/examples/llama.swiftui/llama.swiftui.xcodeproj/project.xcworkspace/xcshareddata:\n",
            "IDEWorkspaceChecks.plist\n",
            "\n",
            "/content/llama.cpp/examples/lookahead:\n",
            "CMakeLists.txt\tlookahead.cpp  README.md\n",
            "\n",
            "/content/llama.cpp/examples/lookup:\n",
            "CMakeLists.txt\tlookup-create.cpp  lookup-stats.cpp\n",
            "lookup.cpp\tlookup-merge.cpp   README.md\n",
            "\n",
            "/content/llama.cpp/examples/model-conversion:\n",
            "CMakeLists.txt\tlogits.cpp  Makefile  README.md  requirements.txt  scripts\n",
            "\n",
            "/content/llama.cpp/examples/model-conversion/scripts:\n",
            "causal\tembedding  utils\n",
            "\n",
            "/content/llama.cpp/examples/model-conversion/scripts/causal:\n",
            "compare-embeddings-logits.sh  run-casual-gen-embeddings-org.py\n",
            "compare-logits.py\t      run-converted-model-embeddings-logits.sh\n",
            "convert-model.sh\t      run-converted-model.sh\n",
            "modelcard.template\t      run-org-model.py\n",
            "\n",
            "/content/llama.cpp/examples/model-conversion/scripts/embedding:\n",
            "compare-embeddings-logits.sh  modelcard.template      run-original-model.py\n",
            "convert-model.sh\t      run-converted-model.sh\n",
            "\n",
            "/content/llama.cpp/examples/model-conversion/scripts/utils:\n",
            "check-nmse.py\t\t\tinspect-org-model.py\n",
            "create-collection-add-model.sh\tperplexity-gen.sh\n",
            "curl-embedding-server.sh\tperplexity-run.sh\n",
            "hf-add-model-to-collection.py\tperplexity-run-simple.sh\n",
            "hf-create-collection.py\t\tquantize.sh\n",
            "hf-create-model.py\t\trun-embedding-server.sh\n",
            "hf-upload-gguf-model.py\t\tsemantic_check.py\n",
            "inspect-converted-model.sh\n",
            "\n",
            "/content/llama.cpp/examples/parallel:\n",
            "CMakeLists.txt\tparallel.cpp  README.md\n",
            "\n",
            "/content/llama.cpp/examples/passkey:\n",
            "CMakeLists.txt\tpasskey.cpp  README.md\n",
            "\n",
            "/content/llama.cpp/examples/retrieval:\n",
            "CMakeLists.txt\tREADME.md  retrieval.cpp\n",
            "\n",
            "/content/llama.cpp/examples/save-load-state:\n",
            "CMakeLists.txt\tsave-load-state.cpp\n",
            "\n",
            "/content/llama.cpp/examples/simple:\n",
            "CMakeLists.txt\tREADME.md  simple.cpp\n",
            "\n",
            "/content/llama.cpp/examples/simple-chat:\n",
            "CMakeLists.txt\tREADME.md  simple-chat.cpp\n",
            "\n",
            "/content/llama.cpp/examples/simple-cmake-pkg:\n",
            "CMakeLists.txt\tREADME.md\n",
            "\n",
            "/content/llama.cpp/examples/speculative:\n",
            "CMakeLists.txt\tREADME.md  speculative.cpp\n",
            "\n",
            "/content/llama.cpp/examples/speculative-simple:\n",
            "CMakeLists.txt\tREADME.md  speculative-simple.cpp\n",
            "\n",
            "/content/llama.cpp/examples/sycl:\n",
            "build.sh\t    README.md\t   win-build-sycl.bat\n",
            "CMakeLists.txt\t    run-llama2.sh  win-run-llama2.bat\n",
            "ls-sycl-device.cpp  run-llama3.sh  win-run-llama3.bat\n",
            "\n",
            "/content/llama.cpp/examples/training:\n",
            "CMakeLists.txt\tfinetune.cpp  README.md\n",
            "\n",
            "/content/llama.cpp/ggml:\n",
            "cmake  CMakeLists.txt  include\tsrc\n",
            "\n",
            "/content/llama.cpp/ggml/cmake:\n",
            "common.cmake  ggml-config.cmake.in  GitVars.cmake\n",
            "\n",
            "/content/llama.cpp/ggml/include:\n",
            "ggml-alloc.h\tggml-cpp.h   ggml-metal.h   ggml-sycl.h    gguf.h\n",
            "ggml-backend.h\tggml-cpu.h   ggml-opencl.h  ggml-vulkan.h\n",
            "ggml-blas.h\tggml-cuda.h  ggml-opt.h     ggml-webgpu.h\n",
            "ggml-cann.h\tggml.h\t     ggml-rpc.h     ggml-zdnn.h\n",
            "\n",
            "/content/llama.cpp/ggml/src:\n",
            "CMakeLists.txt\t      ggml-cann      ggml-metal     ggml-sycl\n",
            "ggml-alloc.c\t      ggml-common.h  ggml-musa\t    ggml-threading.cpp\n",
            "ggml-backend.cpp      ggml.cpp\t     ggml-opencl    ggml-threading.h\n",
            "ggml-backend-impl.h   ggml-cpu\t     ggml-opt.cpp   ggml-vulkan\n",
            "ggml-backend-reg.cpp  ggml-cuda      ggml-quants.c  ggml-webgpu\n",
            "ggml-blas\t      ggml-hip\t     ggml-quants.h  ggml-zdnn\n",
            "ggml.c\t\t      ggml-impl.h    ggml-rpc\t    gguf.cpp\n",
            "\n",
            "/content/llama.cpp/ggml/src/ggml-blas:\n",
            "CMakeLists.txt\tggml-blas.cpp\n",
            "\n",
            "/content/llama.cpp/ggml/src/ggml-cann:\n",
            "aclnn_ops.cpp  acl_tensor.cpp  CMakeLists.txt  Doxyfile\n",
            "aclnn_ops.h    acl_tensor.h    common.h        ggml-cann.cpp\n",
            "\n",
            "/content/llama.cpp/ggml/src/ggml-cpu:\n",
            "amx\t\t CMakeLists.txt   hbm.h      quants.h\t      unary-ops.cpp\n",
            "arch\t\t common.h\t  kleidiai   repack.cpp       unary-ops.h\n",
            "arch-fallback.h  ggml-cpu.c\t  llamafile  repack.h\t      vec.cpp\n",
            "binary-ops.cpp\t ggml-cpu.cpp\t  ops.cpp    simd-mappings.h  vec.h\n",
            "binary-ops.h\t ggml-cpu-impl.h  ops.h      traits.cpp\n",
            "cmake\t\t hbm.cpp\t  quants.c   traits.h\n",
            "\n",
            "/content/llama.cpp/ggml/src/ggml-cpu/amx:\n",
            "amx.cpp  amx.h\tcommon.h  mmq.cpp  mmq.h\n",
            "\n",
            "/content/llama.cpp/ggml/src/ggml-cpu/arch:\n",
            "arm  loongarch\tpowerpc  riscv\ts390  wasm  x86\n",
            "\n",
            "/content/llama.cpp/ggml/src/ggml-cpu/arch/arm:\n",
            "cpu-feats.cpp  quants.c  repack.cpp\n",
            "\n",
            "/content/llama.cpp/ggml/src/ggml-cpu/arch/loongarch:\n",
            "quants.c\n",
            "\n",
            "/content/llama.cpp/ggml/src/ggml-cpu/arch/powerpc:\n",
            "cpu-feats.cpp  quants.c\n",
            "\n",
            "/content/llama.cpp/ggml/src/ggml-cpu/arch/riscv:\n",
            "quants.c  repack.cpp\n",
            "\n",
            "/content/llama.cpp/ggml/src/ggml-cpu/arch/s390:\n",
            "quants.c\n",
            "\n",
            "/content/llama.cpp/ggml/src/ggml-cpu/arch/wasm:\n",
            "quants.c\n",
            "\n",
            "/content/llama.cpp/ggml/src/ggml-cpu/arch/x86:\n",
            "cpu-feats.cpp  quants.c  repack.cpp\n",
            "\n",
            "/content/llama.cpp/ggml/src/ggml-cpu/cmake:\n",
            "FindSIMD.cmake\n",
            "\n",
            "/content/llama.cpp/ggml/src/ggml-cpu/kleidiai:\n",
            "kernels.cpp  kernels.h\tkleidiai.cpp  kleidiai.h\n",
            "\n",
            "/content/llama.cpp/ggml/src/ggml-cpu/llamafile:\n",
            "sgemm.cpp  sgemm.h\n",
            "\n",
            "/content/llama.cpp/ggml/src/ggml-cuda:\n",
            "acc.cu\t\t\tfattn-common.cuh    pad_reflect_1d.cuh\n",
            "acc.cuh\t\t\tfattn.cu\t    pool2d.cu\n",
            "add-id.cu\t\tfattn.cuh\t    pool2d.cuh\n",
            "add-id.cuh\t\tfattn-mma-f16.cuh   quantize.cu\n",
            "arange.cu\t\tfattn-tile.cu\t    quantize.cuh\n",
            "arange.cuh\t\tfattn-tile.cuh\t    reduce_rows.cuh\n",
            "argmax.cu\t\tfattn-vec-f16.cuh   roll.cu\n",
            "argmax.cuh\t\tfattn-vec-f32.cuh   roll.cuh\n",
            "argsort.cu\t\tfattn-wmma-f16.cu   rope.cu\n",
            "argsort.cuh\t\tfattn-wmma-f16.cuh  rope.cuh\n",
            "binbcast.cu\t\tgetrows.cu\t    scale.cu\n",
            "binbcast.cuh\t\tgetrows.cuh\t    scale.cuh\n",
            "clamp.cu\t\tggml-cuda.cu\t    set-rows.cu\n",
            "clamp.cuh\t\tgla.cu\t\t    set-rows.cuh\n",
            "CMakeLists.txt\t\tgla.cuh\t\t    softcap.cu\n",
            "common.cuh\t\tim2col.cu\t    softcap.cuh\n",
            "concat.cu\t\tim2col.cuh\t    softmax.cu\n",
            "concat.cuh\t\tmean.cu\t\t    softmax.cuh\n",
            "conv2d.cu\t\tmean.cuh\t    ssm-conv.cu\n",
            "conv2d.cuh\t\tmma.cuh\t\t    ssm-conv.cuh\n",
            "conv2d-dw.cu\t\tmmf.cu\t\t    ssm-scan.cu\n",
            "conv2d-dw.cuh\t\tmmf.cuh\t\t    ssm-scan.cuh\n",
            "conv2d-transpose.cu\tmmq.cu\t\t    sum.cu\n",
            "conv2d-transpose.cuh\tmmq.cuh\t\t    sum.cuh\n",
            "convert.cu\t\tmmvf.cu\t\t    sumrows.cu\n",
            "convert.cuh\t\tmmvf.cuh\t    sumrows.cuh\n",
            "conv-transpose-1d.cu\tmmvq.cu\t\t    template-instances\n",
            "conv-transpose-1d.cuh\tmmvq.cuh\t    tsembd.cu\n",
            "count-equal.cu\t\tnorm.cu\t\t    tsembd.cuh\n",
            "count-equal.cuh\t\tnorm.cuh\t    unary.cu\n",
            "cp-async.cuh\t\topt-step-adamw.cu   unary.cuh\n",
            "cpy.cu\t\t\topt-step-adamw.cuh  upscale.cu\n",
            "cpy.cuh\t\t\topt-step-sgd.cu     upscale.cuh\n",
            "cpy-utils.cuh\t\topt-step-sgd.cuh    vecdotq.cuh\n",
            "cross-entropy-loss.cu\tout-prod.cu\t    vendors\n",
            "cross-entropy-loss.cuh\tout-prod.cuh\t    wkv.cu\n",
            "dequantize.cuh\t\tpad.cu\t\t    wkv.cuh\n",
            "diagmask.cu\t\tpad.cuh\n",
            "diagmask.cuh\t\tpad_reflect_1d.cu\n",
            "\n",
            "/content/llama.cpp/ggml/src/ggml-cuda/template-instances:\n",
            "fattn-mma-f16-instance-ncols1_16-ncols2_1.cu\n",
            "fattn-mma-f16-instance-ncols1_16-ncols2_2.cu\n",
            "fattn-mma-f16-instance-ncols1_16-ncols2_4.cu\n",
            "fattn-mma-f16-instance-ncols1_1-ncols2_16.cu\n",
            "fattn-mma-f16-instance-ncols1_1-ncols2_8.cu\n",
            "fattn-mma-f16-instance-ncols1_2-ncols2_16.cu\n",
            "fattn-mma-f16-instance-ncols1_2-ncols2_4.cu\n",
            "fattn-mma-f16-instance-ncols1_2-ncols2_8.cu\n",
            "fattn-mma-f16-instance-ncols1_32-ncols2_1.cu\n",
            "fattn-mma-f16-instance-ncols1_32-ncols2_2.cu\n",
            "fattn-mma-f16-instance-ncols1_4-ncols2_16.cu\n",
            "fattn-mma-f16-instance-ncols1_4-ncols2_2.cu\n",
            "fattn-mma-f16-instance-ncols1_4-ncols2_4.cu\n",
            "fattn-mma-f16-instance-ncols1_4-ncols2_8.cu\n",
            "fattn-mma-f16-instance-ncols1_64-ncols2_1.cu\n",
            "fattn-mma-f16-instance-ncols1_8-ncols2_1.cu\n",
            "fattn-mma-f16-instance-ncols1_8-ncols2_2.cu\n",
            "fattn-mma-f16-instance-ncols1_8-ncols2_4.cu\n",
            "fattn-mma-f16-instance-ncols1_8-ncols2_8.cu\n",
            "fattn-vec-f16-instance-hs128-f16-f16.cu\n",
            "fattn-vec-f16-instance-hs128-f16-q4_0.cu\n",
            "fattn-vec-f16-instance-hs128-f16-q4_1.cu\n",
            "fattn-vec-f16-instance-hs128-f16-q5_0.cu\n",
            "fattn-vec-f16-instance-hs128-f16-q5_1.cu\n",
            "fattn-vec-f16-instance-hs128-f16-q8_0.cu\n",
            "fattn-vec-f16-instance-hs128-q4_0-f16.cu\n",
            "fattn-vec-f16-instance-hs128-q4_0-q4_0.cu\n",
            "fattn-vec-f16-instance-hs128-q4_0-q4_1.cu\n",
            "fattn-vec-f16-instance-hs128-q4_0-q5_0.cu\n",
            "fattn-vec-f16-instance-hs128-q4_0-q5_1.cu\n",
            "fattn-vec-f16-instance-hs128-q4_0-q8_0.cu\n",
            "fattn-vec-f16-instance-hs128-q4_1-f16.cu\n",
            "fattn-vec-f16-instance-hs128-q4_1-q4_0.cu\n",
            "fattn-vec-f16-instance-hs128-q4_1-q4_1.cu\n",
            "fattn-vec-f16-instance-hs128-q4_1-q5_0.cu\n",
            "fattn-vec-f16-instance-hs128-q4_1-q5_1.cu\n",
            "fattn-vec-f16-instance-hs128-q4_1-q8_0.cu\n",
            "fattn-vec-f16-instance-hs128-q5_0-f16.cu\n",
            "fattn-vec-f16-instance-hs128-q5_0-q4_0.cu\n",
            "fattn-vec-f16-instance-hs128-q5_0-q4_1.cu\n",
            "fattn-vec-f16-instance-hs128-q5_0-q5_0.cu\n",
            "fattn-vec-f16-instance-hs128-q5_0-q5_1.cu\n",
            "fattn-vec-f16-instance-hs128-q5_0-q8_0.cu\n",
            "fattn-vec-f16-instance-hs128-q5_1-f16.cu\n",
            "fattn-vec-f16-instance-hs128-q5_1-q4_0.cu\n",
            "fattn-vec-f16-instance-hs128-q5_1-q4_1.cu\n",
            "fattn-vec-f16-instance-hs128-q5_1-q5_0.cu\n",
            "fattn-vec-f16-instance-hs128-q5_1-q5_1.cu\n",
            "fattn-vec-f16-instance-hs128-q5_1-q8_0.cu\n",
            "fattn-vec-f16-instance-hs128-q8_0-f16.cu\n",
            "fattn-vec-f16-instance-hs128-q8_0-q4_0.cu\n",
            "fattn-vec-f16-instance-hs128-q8_0-q4_1.cu\n",
            "fattn-vec-f16-instance-hs128-q8_0-q5_0.cu\n",
            "fattn-vec-f16-instance-hs128-q8_0-q5_1.cu\n",
            "fattn-vec-f16-instance-hs128-q8_0-q8_0.cu\n",
            "fattn-vec-f16-instance-hs256-f16-f16.cu\n",
            "fattn-vec-f16-instance-hs64-f16-f16.cu\n",
            "fattn-vec-f16-instance-hs64-f16-q4_0.cu\n",
            "fattn-vec-f16-instance-hs64-f16-q4_1.cu\n",
            "fattn-vec-f16-instance-hs64-f16-q5_0.cu\n",
            "fattn-vec-f16-instance-hs64-f16-q5_1.cu\n",
            "fattn-vec-f16-instance-hs64-f16-q8_0.cu\n",
            "fattn-vec-f32-instance-hs128-f16-f16.cu\n",
            "fattn-vec-f32-instance-hs128-f16-q4_0.cu\n",
            "fattn-vec-f32-instance-hs128-f16-q4_1.cu\n",
            "fattn-vec-f32-instance-hs128-f16-q5_0.cu\n",
            "fattn-vec-f32-instance-hs128-f16-q5_1.cu\n",
            "fattn-vec-f32-instance-hs128-f16-q8_0.cu\n",
            "fattn-vec-f32-instance-hs128-q4_0-f16.cu\n",
            "fattn-vec-f32-instance-hs128-q4_0-q4_0.cu\n",
            "fattn-vec-f32-instance-hs128-q4_0-q4_1.cu\n",
            "fattn-vec-f32-instance-hs128-q4_0-q5_0.cu\n",
            "fattn-vec-f32-instance-hs128-q4_0-q5_1.cu\n",
            "fattn-vec-f32-instance-hs128-q4_0-q8_0.cu\n",
            "fattn-vec-f32-instance-hs128-q4_1-f16.cu\n",
            "fattn-vec-f32-instance-hs128-q4_1-q4_0.cu\n",
            "fattn-vec-f32-instance-hs128-q4_1-q4_1.cu\n",
            "fattn-vec-f32-instance-hs128-q4_1-q5_0.cu\n",
            "fattn-vec-f32-instance-hs128-q4_1-q5_1.cu\n",
            "fattn-vec-f32-instance-hs128-q4_1-q8_0.cu\n",
            "fattn-vec-f32-instance-hs128-q5_0-f16.cu\n",
            "fattn-vec-f32-instance-hs128-q5_0-q4_0.cu\n",
            "fattn-vec-f32-instance-hs128-q5_0-q4_1.cu\n",
            "fattn-vec-f32-instance-hs128-q5_0-q5_0.cu\n",
            "fattn-vec-f32-instance-hs128-q5_0-q5_1.cu\n",
            "fattn-vec-f32-instance-hs128-q5_0-q8_0.cu\n",
            "fattn-vec-f32-instance-hs128-q5_1-f16.cu\n",
            "fattn-vec-f32-instance-hs128-q5_1-q4_0.cu\n",
            "fattn-vec-f32-instance-hs128-q5_1-q4_1.cu\n",
            "fattn-vec-f32-instance-hs128-q5_1-q5_0.cu\n",
            "fattn-vec-f32-instance-hs128-q5_1-q5_1.cu\n",
            "fattn-vec-f32-instance-hs128-q5_1-q8_0.cu\n",
            "fattn-vec-f32-instance-hs128-q8_0-f16.cu\n",
            "fattn-vec-f32-instance-hs128-q8_0-q4_0.cu\n",
            "fattn-vec-f32-instance-hs128-q8_0-q4_1.cu\n",
            "fattn-vec-f32-instance-hs128-q8_0-q5_0.cu\n",
            "fattn-vec-f32-instance-hs128-q8_0-q5_1.cu\n",
            "fattn-vec-f32-instance-hs128-q8_0-q8_0.cu\n",
            "fattn-vec-f32-instance-hs256-f16-f16.cu\n",
            "fattn-vec-f32-instance-hs64-f16-f16.cu\n",
            "fattn-vec-f32-instance-hs64-f16-q4_0.cu\n",
            "fattn-vec-f32-instance-hs64-f16-q4_1.cu\n",
            "fattn-vec-f32-instance-hs64-f16-q5_0.cu\n",
            "fattn-vec-f32-instance-hs64-f16-q5_1.cu\n",
            "fattn-vec-f32-instance-hs64-f16-q8_0.cu\n",
            "generate_cu_files.py\n",
            "mmf-instance-ncols_10.cu\n",
            "mmf-instance-ncols_11.cu\n",
            "mmf-instance-ncols_12.cu\n",
            "mmf-instance-ncols_13.cu\n",
            "mmf-instance-ncols_14.cu\n",
            "mmf-instance-ncols_15.cu\n",
            "mmf-instance-ncols_16.cu\n",
            "mmf-instance-ncols_1.cu\n",
            "mmf-instance-ncols_2.cu\n",
            "mmf-instance-ncols_3.cu\n",
            "mmf-instance-ncols_4.cu\n",
            "mmf-instance-ncols_5.cu\n",
            "mmf-instance-ncols_6.cu\n",
            "mmf-instance-ncols_7.cu\n",
            "mmf-instance-ncols_8.cu\n",
            "mmf-instance-ncols_9.cu\n",
            "mmq-instance-iq1_s.cu\n",
            "mmq-instance-iq2_s.cu\n",
            "mmq-instance-iq2_xs.cu\n",
            "mmq-instance-iq2_xxs.cu\n",
            "mmq-instance-iq3_s.cu\n",
            "mmq-instance-iq3_xxs.cu\n",
            "mmq-instance-iq4_nl.cu\n",
            "mmq-instance-iq4_xs.cu\n",
            "mmq-instance-mxfp4.cu\n",
            "mmq-instance-q2_k.cu\n",
            "mmq-instance-q3_k.cu\n",
            "mmq-instance-q4_0.cu\n",
            "mmq-instance-q4_1.cu\n",
            "mmq-instance-q4_k.cu\n",
            "mmq-instance-q5_0.cu\n",
            "mmq-instance-q5_1.cu\n",
            "mmq-instance-q5_k.cu\n",
            "mmq-instance-q6_k.cu\n",
            "mmq-instance-q8_0.cu\n",
            "\n",
            "/content/llama.cpp/ggml/src/ggml-cuda/vendors:\n",
            "cuda.h\thip.h  musa.h\n",
            "\n",
            "/content/llama.cpp/ggml/src/ggml-hip:\n",
            "CMakeLists.txt\n",
            "\n",
            "/content/llama.cpp/ggml/src/ggml-metal:\n",
            "CMakeLists.txt\t       ggml-metal-common.h  ggml-metal.m\n",
            "ggml-metal-common.cpp  ggml-metal-impl.h    ggml-metal.metal\n",
            "\n",
            "/content/llama.cpp/ggml/src/ggml-musa:\n",
            "CMakeLists.txt\tmudnn.cu  mudnn.cuh\n",
            "\n",
            "/content/llama.cpp/ggml/src/ggml-opencl:\n",
            "CMakeLists.txt\tggml-opencl.cpp  kernels\n",
            "\n",
            "/content/llama.cpp/ggml/src/ggml-opencl/kernels:\n",
            "add.cl\t\t\t   im2col_f16.cl\t\t   norm.cl\n",
            "add_id.cl\t\t   im2col_f32.cl\t\t   pad.cl\n",
            "argsort.cl\t\t   mul.cl\t\t\t   relu.cl\n",
            "clamp.cl\t\t   mul_mat_Ab_Bi_8x4.cl\t\t   repeat.cl\n",
            "concat.cl\t\t   mul_mat_f16_f32.cl\t\t   rms_norm.cl\n",
            "conv2d.cl\t\t   mul_mm_f16_f32_l4_lm.cl\t   rope.cl\n",
            "conv2d_f16_f32.cl\t   mul_mm_f32_f32_l4_lm.cl\t   scale.cl\n",
            "cpy.cl\t\t\t   mul_mv_f16_f16.cl\t\t   set_rows.cl\n",
            "cvt.cl\t\t\t   mul_mv_f16_f32_1row.cl\t   sigmoid.cl\n",
            "diag_mask_inf.cl\t   mul_mv_f16_f32.cl\t\t   silu.cl\n",
            "div.cl\t\t\t   mul_mv_f16_f32_l4.cl\t\t   softmax_4_f16.cl\n",
            "embed_kernel.py\t\t   mul_mv_f32_f32.cl\t\t   softmax_4_f32.cl\n",
            "flash_attn_f16.cl\t   mul_mv_id_mxfp4_f32.cl\t   softmax_f16.cl\n",
            "flash_attn_f32.cl\t   mul_mv_id_q4_0_f32_8x_flat.cl   softmax_f32.cl\n",
            "flash_attn_f32_f16.cl\t   mul_mv_mxfp4_f32.cl\t\t   sub.cl\n",
            "gelu.cl\t\t\t   mul_mv_q4_0_f32_1d_16x_flat.cl  sum_rows.cl\n",
            "gemv_noshuffle.cl\t   mul_mv_q4_0_f32_1d_8x_flat.cl   tanh.cl\n",
            "gemv_noshuffle_general.cl  mul_mv_q4_0_f32_8x_flat.cl\t   transpose.cl\n",
            "get_rows.cl\t\t   mul_mv_q4_0_f32.cl\t\t   tsembd.cl\n",
            "glu.cl\t\t\t   mul_mv_q4_0_f32_v.cl\t\t   upscale.cl\n",
            "group_norm.cl\t\t   mul_mv_q6_k.cl\n",
            "\n",
            "/content/llama.cpp/ggml/src/ggml-rpc:\n",
            "CMakeLists.txt\tggml-rpc.cpp\n",
            "\n",
            "/content/llama.cpp/ggml/src/ggml-sycl:\n",
            "backend.hpp\tconv.hpp\t  getrows.hpp\t norm.hpp      softmax.hpp\n",
            "binbcast.cpp\tcpy.cpp\t\t  ggml-sycl.cpp  outprod.cpp   sycl_hw.cpp\n",
            "binbcast.hpp\tcpy.hpp\t\t  gla.cpp\t outprod.hpp   sycl_hw.hpp\n",
            "CMakeLists.txt\tdequantize.hpp\t  gla.hpp\t presets.hpp   tsembd.cpp\n",
            "common.cpp\tdmmv.cpp\t  im2col.cpp\t quantize.hpp  tsembd.hpp\n",
            "common.hpp\tdmmv.hpp\t  im2col.hpp\t quants.hpp    vecdotq.hpp\n",
            "concat.cpp\tdpct\t\t  mmq.cpp\t rope.cpp      wkv.cpp\n",
            "concat.hpp\telement_wise.cpp  mmq.hpp\t rope.hpp      wkv.hpp\n",
            "conv.cpp\telement_wise.hpp  mmvq.cpp\t set_rows.cpp\n",
            "convert.cpp\tgemm.hpp\t  mmvq.hpp\t set_rows.hpp\n",
            "convert.hpp\tgetrows.cpp\t  norm.cpp\t softmax.cpp\n",
            "\n",
            "/content/llama.cpp/ggml/src/ggml-sycl/dpct:\n",
            "helper.hpp\n",
            "\n",
            "/content/llama.cpp/ggml/src/ggml-vulkan:\n",
            "cmake  CMakeLists.txt  ggml-vulkan.cpp\tvulkan-shaders\n",
            "\n",
            "/content/llama.cpp/ggml/src/ggml-vulkan/cmake:\n",
            "host-toolchain.cmake.in\n",
            "\n",
            "/content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders:\n",
            "acc.comp\t\t\tmul_mat_split_k_reduce.comp\n",
            "add.comp\t\t\tmul_mat_vec_base.comp\n",
            "add_id.comp\t\t\tmul_mat_vec.comp\n",
            "argmax.comp\t\t\tmul_mat_vec_iq1_m.comp\n",
            "argsort.comp\t\t\tmul_mat_vec_iq1_s.comp\n",
            "clamp.comp\t\t\tmul_mat_vec_iq2_s.comp\n",
            "CMakeLists.txt\t\t\tmul_mat_vec_iq2_xs.comp\n",
            "concat.comp\t\t\tmul_mat_vec_iq2_xxs.comp\n",
            "contig_copy.comp\t\tmul_mat_vec_iq3_s.comp\n",
            "conv2d_dw.comp\t\t\tmul_mat_vec_iq3_xxs.comp\n",
            "conv2d_mm.comp\t\t\tmul_mat_vec_nc.comp\n",
            "conv_transpose_1d.comp\t\tmul_mat_vec_p021.comp\n",
            "copy.comp\t\t\tmul_mat_vec_q2_k.comp\n",
            "copy_from_quant.comp\t\tmul_mat_vec_q3_k.comp\n",
            "copy_to_quant.comp\t\tmul_mat_vec_q4_k.comp\n",
            "cos.comp\t\t\tmul_mat_vec_q5_k.comp\n",
            "count_equal.comp\t\tmul_mat_vec_q6_k.comp\n",
            "dequant_f32.comp\t\tmul_mat_vecq.comp\n",
            "dequant_funcs_cm2.comp\t\tmul_mm_cm2.comp\n",
            "dequant_funcs.comp\t\tmul_mm.comp\n",
            "dequant_head.comp\t\tmul_mm_funcs.comp\n",
            "dequant_iq1_m.comp\t\tmul_mmq.comp\n",
            "dequant_iq1_s.comp\t\tmul_mmq_funcs.comp\n",
            "dequant_iq2_s.comp\t\tmulti_add.comp\n",
            "dequant_iq2_xs.comp\t\tnorm.comp\n",
            "dequant_iq2_xxs.comp\t\topt_step_adamw.comp\n",
            "dequant_iq3_s.comp\t\topt_step_sgd.comp\n",
            "dequant_iq3_xxs.comp\t\tpad.comp\n",
            "dequant_iq4_nl.comp\t\tpool2d.comp\n",
            "dequant_iq4_xs.comp\t\tquantize_q8_1.comp\n",
            "dequant_mxfp4.comp\t\treglu.comp\n",
            "dequant_q2_k.comp\t\trelu.comp\n",
            "dequant_q3_k.comp\t\trepeat_back.comp\n",
            "dequant_q4_0.comp\t\trepeat.comp\n",
            "dequant_q4_1.comp\t\trms_norm_back.comp\n",
            "dequant_q4_k.comp\t\trms_norm.comp\n",
            "dequant_q5_0.comp\t\trms_norm_partials.comp\n",
            "dequant_q5_1.comp\t\troll.comp\n",
            "dequant_q5_k.comp\t\trope_head.comp\n",
            "dequant_q6_k.comp\t\trope_multi.comp\n",
            "dequant_q8_0.comp\t\trope_neox.comp\n",
            "diag_mask_inf.comp\t\trope_norm.comp\n",
            "div.comp\t\t\trope_vision.comp\n",
            "exp.comp\t\t\trte.comp\n",
            "flash_attn_base.comp\t\tscale.comp\n",
            "flash_attn_cm1.comp\t\tsigmoid.comp\n",
            "flash_attn_cm2.comp\t\tsilu_back.comp\n",
            "flash_attn.comp\t\t\tsilu.comp\n",
            "flash_attn_split_k_reduce.comp\tsin.comp\n",
            "geglu.comp\t\t\tsoft_max_back.comp\n",
            "geglu_erf.comp\t\t\tsoft_max.comp\n",
            "geglu_quick.comp\t\tsqrt.comp\n",
            "gelu.comp\t\t\tsquare.comp\n",
            "gelu_erf.comp\t\t\tsub.comp\n",
            "gelu_quick.comp\t\t\tsum_rows.comp\n",
            "generic_binary_head.comp\tswiglu.comp\n",
            "generic_head.comp\t\tswiglu_oai.comp\n",
            "generic_unary_head.comp\t\ttanh.comp\n",
            "get_rows.comp\t\t\ttest_bfloat16_support.comp\n",
            "get_rows_quant.comp\t\ttest_coopmat2_support.comp\n",
            "glu_head.comp\t\t\ttest_coopmat_support.comp\n",
            "glu_main.comp\t\t\ttest_integer_dot_support.comp\n",
            "group_norm.comp\t\t\ttimestep_embedding.comp\n",
            "hardsigmoid.comp\t\ttypes.comp\n",
            "hardswish.comp\t\t\tupscale.comp\n",
            "im2col_3d.comp\t\t\tutils.comp\n",
            "im2col.comp\t\t\tvulkan-shaders-gen.cpp\n",
            "l2_norm.comp\t\t\twkv6.comp\n",
            "leaky_relu.comp\t\t\twkv7.comp\n",
            "mul.comp\n",
            "\n",
            "/content/llama.cpp/ggml/src/ggml-webgpu:\n",
            "CMakeLists.txt\tggml-webgpu.cpp  wgsl-shaders\n",
            "\n",
            "/content/llama.cpp/ggml/src/ggml-webgpu/wgsl-shaders:\n",
            "cpy.wgsl  embed_wgsl.py  memset.wgsl  mul_mat.tmpl.wgsl  set_rows.wgsl\n",
            "\n",
            "/content/llama.cpp/ggml/src/ggml-zdnn:\n",
            "CMakeLists.txt\tggml-zdnn.cpp  ggml-zdnn-impl.h\n",
            "\n",
            "/content/llama.cpp/gguf-py:\n",
            "examples  gguf\tLICENSE  pyproject.toml  README.md  tests\n",
            "\n",
            "/content/llama.cpp/gguf-py/examples:\n",
            "reader.py  writer.py\n",
            "\n",
            "/content/llama.cpp/gguf-py/gguf:\n",
            "constants.py\tgguf_writer.py\tmetadata.py  scripts\t\tvocab.py\n",
            "gguf.py\t\t__init__.py\tpy.typed     tensor_mapping.py\n",
            "gguf_reader.py\tlazy.py\t\tquants.py    utility.py\n",
            "\n",
            "/content/llama.cpp/gguf-py/gguf/scripts:\n",
            "gguf_convert_endian.py\tgguf_editor_gui.py  gguf_new_metadata.py\n",
            "gguf_dump.py\t\tgguf_hash.py\t    gguf_set_metadata.py\n",
            "\n",
            "/content/llama.cpp/gguf-py/tests:\n",
            "__init__.py  test_metadata.py  test_quants.py\n",
            "\n",
            "/content/llama.cpp/grammars:\n",
            "arithmetic.gbnf  chess.gbnf    japanese.gbnf  json.gbnf  README.md\n",
            "c.gbnf\t\t english.gbnf  json_arr.gbnf  list.gbnf\n",
            "\n",
            "/content/llama.cpp/include:\n",
            "llama-cpp.h  llama.h\n",
            "\n",
            "/content/llama.cpp/licenses:\n",
            "LICENSE-curl  LICENSE-httplib  LICENSE-jsonhpp\tLICENSE-linenoise\n",
            "\n",
            "/content/llama.cpp/media:\n",
            "llama0-banner.png  llama1-icon.svg\t\tllama1-logo.svg\n",
            "llama0-logo.png    llama1-icon-transparent.png\tmatmul.png\n",
            "llama1-banner.png  llama1-icon-transparent.svg\tmatmul.svg\n",
            "llama1-icon.png    llama1-logo.png\n",
            "\n",
            "/content/llama.cpp/models:\n",
            "ggml-vocab-aquila.gguf\t\t    ggml-vocab-llama-bpe.gguf.inp\n",
            "ggml-vocab-baichuan.gguf\t    ggml-vocab-llama-bpe.gguf.out\n",
            "ggml-vocab-bert-bge.gguf\t    ggml-vocab-llama-spm.gguf\n",
            "ggml-vocab-bert-bge.gguf.inp\t    ggml-vocab-llama-spm.gguf.inp\n",
            "ggml-vocab-bert-bge.gguf.out\t    ggml-vocab-llama-spm.gguf.out\n",
            "ggml-vocab-command-r.gguf\t    ggml-vocab-mpt.gguf\n",
            "ggml-vocab-command-r.gguf.inp\t    ggml-vocab-mpt.gguf.inp\n",
            "ggml-vocab-command-r.gguf.out\t    ggml-vocab-mpt.gguf.out\n",
            "ggml-vocab-deepseek-coder.gguf\t    ggml-vocab-nomic-bert-moe.gguf\n",
            "ggml-vocab-deepseek-coder.gguf.inp  ggml-vocab-phi-3.gguf\n",
            "ggml-vocab-deepseek-coder.gguf.out  ggml-vocab-phi-3.gguf.inp\n",
            "ggml-vocab-deepseek-llm.gguf\t    ggml-vocab-phi-3.gguf.out\n",
            "ggml-vocab-deepseek-llm.gguf.inp    ggml-vocab-qwen2.gguf\n",
            "ggml-vocab-deepseek-llm.gguf.out    ggml-vocab-qwen2.gguf.inp\n",
            "ggml-vocab-falcon.gguf\t\t    ggml-vocab-qwen2.gguf.out\n",
            "ggml-vocab-falcon.gguf.inp\t    ggml-vocab-refact.gguf\n",
            "ggml-vocab-falcon.gguf.out\t    ggml-vocab-refact.gguf.inp\n",
            "ggml-vocab-gpt-2.gguf\t\t    ggml-vocab-refact.gguf.out\n",
            "ggml-vocab-gpt-2.gguf.inp\t    ggml-vocab-starcoder.gguf\n",
            "ggml-vocab-gpt-2.gguf.out\t    ggml-vocab-starcoder.gguf.inp\n",
            "ggml-vocab-gpt-neox.gguf\t    ggml-vocab-starcoder.gguf.out\n",
            "ggml-vocab-llama-bpe.gguf\t    templates\n",
            "\n",
            "/content/llama.cpp/models/templates:\n",
            "ByteDance-Seed-OSS.jinja\n",
            "CohereForAI-c4ai-command-r7b-12-2024-tool_use.jinja\n",
            "CohereForAI-c4ai-command-r-plus-tool_use.jinja\n",
            "deepseek-ai-DeepSeek-R1-Distill-Llama-8B.jinja\n",
            "deepseek-ai-DeepSeek-R1-Distill-Qwen-32B.jinja\n",
            "deepseek-ai-DeepSeek-V3.1.jinja\n",
            "fireworks-ai-llama-3-firefunction-v2.jinja\n",
            "google-gemma-2-2b-it.jinja\n",
            "ibm-granite-granite-3.3-2B-Instruct.jinja\n",
            "llama-cpp-deepseek-r1.jinja\n",
            "llama-cpp-rwkv-world.jinja\n",
            "meetkai-functionary-medium-v3.1.jinja\n",
            "meetkai-functionary-medium-v3.2.jinja\n",
            "meta-llama-Llama-3.1-8B-Instruct.jinja\n",
            "meta-llama-Llama-3.2-3B-Instruct.jinja\n",
            "meta-llama-Llama-3.3-70B-Instruct.jinja\n",
            "microsoft-Phi-3.5-mini-instruct.jinja\n",
            "mistralai-Mistral-Nemo-Instruct-2407.jinja\n",
            "Mistral-Small-3.2-24B-Instruct-2506.jinja\n",
            "moonshotai-Kimi-K2.jinja\n",
            "NousResearch-Hermes-2-Pro-Llama-3-8B-tool_use.jinja\n",
            "NousResearch-Hermes-3-Llama-3.1-8B-tool_use.jinja\n",
            "NVIDIA-Nemotron-Nano-v2.jinja\n",
            "openai-gpt-oss-120b.jinja\n",
            "Qwen-Qwen2.5-7B-Instruct.jinja\n",
            "Qwen-Qwen3-0.6B.jinja\n",
            "Qwen-QwQ-32B.jinja\n",
            "README.md\n",
            "unsloth-mistral-Devstral-Small-2507.jinja\n",
            "\n",
            "/content/llama.cpp/pocs:\n",
            "CMakeLists.txt\tvdot\n",
            "\n",
            "/content/llama.cpp/pocs/vdot:\n",
            "CMakeLists.txt\tq8dot.cpp  vdot.cpp\n",
            "\n",
            "/content/llama.cpp/prompts:\n",
            "alpaca.txt\t\tchat-with-qwen.txt\t LLM-questions.txt\n",
            "assistant.txt\t\tchat-with-vicuna-v0.txt  mnemonics.txt\n",
            "chat.txt\t\tchat-with-vicuna-v1.txt  parallel-questions.txt\n",
            "chat-with-baichuan.txt\tdan-modified.txt\t reason-act.txt\n",
            "chat-with-bob.txt\tdan.txt\n",
            "\n",
            "/content/llama.cpp/requirements:\n",
            "requirements-all.txt\n",
            "requirements-compare-llama-bench.txt\n",
            "requirements-convert_hf_to_gguf.txt\n",
            "requirements-convert_hf_to_gguf_update.txt\n",
            "requirements-convert_legacy_llama.txt\n",
            "requirements-convert_llama_ggml_to_gguf.txt\n",
            "requirements-convert_lora_to_gguf.txt\n",
            "requirements-gguf_editor_gui.txt\n",
            "requirements-pydantic.txt\n",
            "requirements-server-bench.txt\n",
            "requirements-test-tokenizer-random.txt\n",
            "requirements-tool_bench.txt\n",
            "\n",
            "/content/llama.cpp/scripts:\n",
            "apple\t\t\t     get_chat_template.py  run-all-perf.sh\n",
            "build-info.sh\t\t     get-flags.mk\t   run-all-ppl.sh\n",
            "check-requirements.sh\t     get-hellaswag.sh\t   server-bench.py\n",
            "ci-run.sh\t\t     get-pg.sh\t\t   sync-ggml-am.sh\n",
            "compare-commits.sh\t     get-wikitext-103.sh   sync-ggml.last\n",
            "compare-llama-bench.py\t     get-wikitext-2.sh\t   sync-ggml.sh\n",
            "create_ops_docs.py\t     get-winogrande.sh\t   sync_vendor.py\n",
            "debug-test.sh\t\t     hf.sh\t\t   tool_bench.py\n",
            "fetch_server_test_models.py  install-oneapi.bat    tool_bench.sh\n",
            "gen-authors.sh\t\t     jinja\t\t   verify-checksum-models.py\n",
            "gen-unicode-data.py\t     qnt-all.sh\t\t   xxd.cmake\n",
            "\n",
            "/content/llama.cpp/scripts/apple:\n",
            "validate-apps.sh  validate-macos.sh  validate-visionos.sh\n",
            "validate-ios.sh   validate-tvos.sh\n",
            "\n",
            "/content/llama.cpp/scripts/jinja:\n",
            "jinja-tester.py  requirements.txt\n",
            "\n",
            "/content/llama.cpp/src:\n",
            "CMakeLists.txt\t   llama-hparams.cpp\t       llama-mmap.h\n",
            "llama-adapter.cpp  llama-hparams.h\t       llama-model.cpp\n",
            "llama-adapter.h    llama-impl.cpp\t       llama-model.h\n",
            "llama-arch.cpp\t   llama-impl.h\t\t       llama-model-loader.cpp\n",
            "llama-arch.h\t   llama-io.cpp\t\t       llama-model-loader.h\n",
            "llama-batch.cpp    llama-io.h\t\t       llama-model-saver.cpp\n",
            "llama-batch.h\t   llama-kv-cache.cpp\t       llama-model-saver.h\n",
            "llama-chat.cpp\t   llama-kv-cache.h\t       llama-quant.cpp\n",
            "llama-chat.h\t   llama-kv-cache-iswa.cpp     llama-quant.h\n",
            "llama-context.cpp  llama-kv-cache-iswa.h       llama-sampling.cpp\n",
            "llama-context.h    llama-kv-cells.h\t       llama-sampling.h\n",
            "llama-cparams.cpp  llama-memory.cpp\t       llama-vocab.cpp\n",
            "llama-cparams.h    llama-memory.h\t       llama-vocab.h\n",
            "llama.cpp\t   llama-memory-hybrid.cpp     unicode.cpp\n",
            "llama-grammar.cpp  llama-memory-hybrid.h       unicode-data.cpp\n",
            "llama-grammar.h    llama-memory-recurrent.cpp  unicode-data.h\n",
            "llama-graph.cpp    llama-memory-recurrent.h    unicode.h\n",
            "llama-graph.h\t   llama-mmap.cpp\n",
            "\n",
            "/content/llama.cpp/tests:\n",
            "CMakeLists.txt\t\t\t test-llama-grammar.cpp\n",
            "get-model.cpp\t\t\t test-log.cpp\n",
            "get-model.h\t\t\t test-lora-conversion-inference.sh\n",
            "run-json-schema-to-grammar.mjs\t test-model-load-cancel.cpp\n",
            "test-arg-parser.cpp\t\t test-mtmd-c-api.c\n",
            "test-autorelease.cpp\t\t test-opt.cpp\n",
            "test-backend-ops.cpp\t\t test-quantize-fns.cpp\n",
            "test-barrier.cpp\t\t test-quantize-perf.cpp\n",
            "test-c.c\t\t\t test-quantize-stats.cpp\n",
            "test-chat.cpp\t\t\t test-regex-partial.cpp\n",
            "test-chat-parser.cpp\t\t test-rope.cpp\n",
            "test-chat-template.cpp\t\t test-sampling.cpp\n",
            "test-double-float.cpp\t\t test-thread-safety.cpp\n",
            "test-gbnf-validator.cpp\t\t test-tokenizer-0.cpp\n",
            "test-gguf.cpp\t\t\t test-tokenizer-0.py\n",
            "test-grammar-integration.cpp\t test-tokenizer-0.sh\n",
            "test-grammar-llguidance.cpp\t test-tokenizer-1-bpe.cpp\n",
            "test-grammar-parser.cpp\t\t test-tokenizer-1-spm.cpp\n",
            "test-json-partial.cpp\t\t test-tokenizer-random.py\n",
            "test-json-schema-to-grammar.cpp  test-tokenizers-repo.sh\n",
            "\n",
            "/content/llama.cpp/tools:\n",
            "batched-bench\t   export-lora\tllama-bench  perplexity  run\t   tts\n",
            "CMakeLists.txt\t   gguf-split\tmain\t     quantize\t server\n",
            "cvector-generator  imatrix\tmtmd\t     rpc\t tokenize\n",
            "\n",
            "/content/llama.cpp/tools/batched-bench:\n",
            "batched-bench.cpp  CMakeLists.txt  README.md\n",
            "\n",
            "/content/llama.cpp/tools/cvector-generator:\n",
            "CMakeLists.txt\t cvector-generator.cpp\tnegative.txt  positive.txt\n",
            "completions.txt  mean.hpp\t\tpca.hpp       README.md\n",
            "\n",
            "/content/llama.cpp/tools/export-lora:\n",
            "CMakeLists.txt\texport-lora.cpp  README.md\n",
            "\n",
            "/content/llama.cpp/tools/gguf-split:\n",
            "CMakeLists.txt\tgguf-split.cpp\tREADME.md  tests.sh\n",
            "\n",
            "/content/llama.cpp/tools/imatrix:\n",
            "CMakeLists.txt\timatrix.cpp  README.md\n",
            "\n",
            "/content/llama.cpp/tools/llama-bench:\n",
            "CMakeLists.txt\tllama-bench.cpp  README.md\n",
            "\n",
            "/content/llama.cpp/tools/main:\n",
            "CMakeLists.txt\tmain.cpp  README.md\n",
            "\n",
            "/content/llama.cpp/tools/mtmd:\n",
            "clip.cpp\t\t legacy-models\t mtmd.h\t\t   test-1.jpeg\n",
            "clip.h\t\t\t mtmd-audio.cpp  mtmd-helper.cpp   test-2.mp3\n",
            "clip-impl.h\t\t mtmd-audio.h\t mtmd-helper.h\t   tests.sh\n",
            "CMakeLists.txt\t\t mtmd-cli.cpp\t README.md\n",
            "deprecation-warning.cpp  mtmd.cpp\t requirements.txt\n",
            "\n",
            "/content/llama.cpp/tools/mtmd/legacy-models:\n",
            "convert_image_encoder_to_gguf.py\n",
            "glmedge-convert-image-encoder-to-gguf.py\n",
            "glmedge-surgery.py\n",
            "llava_surgery.py\n",
            "llava_surgery_v2.py\n",
            "minicpmv-convert-image-encoder-to-gguf.py\n",
            "minicpmv-surgery.py\n",
            "\n",
            "/content/llama.cpp/tools/perplexity:\n",
            "CMakeLists.txt\tperplexity.cpp\tREADME.md\n",
            "\n",
            "/content/llama.cpp/tools/quantize:\n",
            "CMakeLists.txt\tquantize.cpp  README.md  tests.sh\n",
            "\n",
            "/content/llama.cpp/tools/rpc:\n",
            "CMakeLists.txt\tREADME.md  rpc-server.cpp\n",
            "\n",
            "/content/llama.cpp/tools/run:\n",
            "CMakeLists.txt\tlinenoise.cpp  README.md  run.cpp\n",
            "\n",
            "/content/llama.cpp/tools/run/linenoise.cpp:\n",
            "linenoise.cpp  linenoise.h\n",
            "\n",
            "/content/llama.cpp/tools/server:\n",
            "bench\t\tchat.sh\t\tpublic_legacy\t   server.cpp  utils.hpp\n",
            "chat-llama2.sh\tCMakeLists.txt\tpublic_simplechat  tests       webui\n",
            "chat.mjs\tpublic\t\tREADME.md\t   themes\n",
            "\n",
            "/content/llama.cpp/tools/server/bench:\n",
            "bench.py  prometheus.yml  README.md  requirements.txt  script.js\n",
            "\n",
            "/content/llama.cpp/tools/server/public:\n",
            "index.html.gz  loading.html\n",
            "\n",
            "/content/llama.cpp/tools/server/public_legacy:\n",
            "colorthemes.css  json-schema-to-grammar.mjs  theme-ketivah.css\n",
            "completion.js\t loading.html\t\t     theme-mangotango.css\n",
            "favicon.ico\t prompt-formats.js\t     theme-playground.css\n",
            "index.html\t style.css\t\t     theme-polarnight.css\n",
            "index.js\t system-prompts.js\t     theme-snowstorm.css\n",
            "index-new.html\t theme-beeninorder.css\n",
            "\n",
            "/content/llama.cpp/tools/server/public_simplechat:\n",
            "datautils.mjs  readme.md       simplechat.js\t\tui.mjs\n",
            "index.html     simplechat.css  simplechat_screens.webp\n",
            "\n",
            "/content/llama.cpp/tools/server/tests:\n",
            "conftest.py  pytest.ini  README.md  requirements.txt  tests.sh\tunit  utils.py\n",
            "\n",
            "/content/llama.cpp/tools/server/tests/unit:\n",
            "test_basic.py\t\t test_infill.py     test_speculative.py\n",
            "test_chat_completion.py  test_lora.py\t    test_template.py\n",
            "test_completion.py\t test_rerank.py     test_tokenize.py\n",
            "test_ctx_shift.py\t test_security.py   test_tool_call.py\n",
            "test_embedding.py\t test_slot_save.py  test_vision_api.py\n",
            "\n",
            "/content/llama.cpp/tools/server/themes:\n",
            "buttons-top  README.md\twild\n",
            "\n",
            "/content/llama.cpp/tools/server/themes/buttons-top:\n",
            "buttons_top.png  favicon.ico  index.html  README.md\n",
            "\n",
            "/content/llama.cpp/tools/server/themes/wild:\n",
            "favicon.ico  index.html  llama_cpp.png\tllamapattern.png  README.md  wild.png\n",
            "\n",
            "/content/llama.cpp/tools/server/webui:\n",
            "eslint.config.js  package-lock.json  src\t\t tsconfig.json\n",
            "index.html\t  postcss.config.js  tailwind.config.js  tsconfig.node.json\n",
            "package.json\t  public\t     tsconfig.app.json\t vite.config.ts\n",
            "\n",
            "/content/llama.cpp/tools/server/webui/public:\n",
            "demo-conversation.json\n",
            "\n",
            "/content/llama.cpp/tools/server/webui/src:\n",
            "App.tsx  components  Config.ts\tindex.scss  main.tsx  utils  vite-env.d.ts\n",
            "\n",
            "/content/llama.cpp/tools/server/webui/src/components:\n",
            "CanvasPyInterpreter.tsx        Header.tsx\t    Sidebar.tsx\n",
            "ChatInputExtraContextItem.tsx  MarkdownDisplay.tsx  useChatExtraContext.tsx\n",
            "ChatMessage.tsx\t\t       ModalProvider.tsx    useChatScroll.tsx\n",
            "ChatScreen.tsx\t\t       SettingDialog.tsx    useChatTextarea.ts\n",
            "\n",
            "/content/llama.cpp/tools/server/webui/src/utils:\n",
            "app.context.tsx  common.tsx  llama-vscode.ts  misc.ts  storage.ts  types.ts\n",
            "\n",
            "/content/llama.cpp/tools/tokenize:\n",
            "CMakeLists.txt\ttokenize.cpp\n",
            "\n",
            "/content/llama.cpp/tools/tts:\n",
            "CMakeLists.txt\tconvert_pt_to_hf.py  README.md\ttts.cpp  tts-outetts.py\n",
            "\n",
            "/content/llama.cpp/vendor:\n",
            "cpp-httplib  miniaudio\tminja  nlohmann  stb\n",
            "\n",
            "/content/llama.cpp/vendor/cpp-httplib:\n",
            "httplib.h\n",
            "\n",
            "/content/llama.cpp/vendor/miniaudio:\n",
            "miniaudio.h\n",
            "\n",
            "/content/llama.cpp/vendor/minja:\n",
            "chat-template.hpp  minja.hpp\n",
            "\n",
            "/content/llama.cpp/vendor/nlohmann:\n",
            "json_fwd.hpp  json.hpp\n",
            "\n",
            "/content/llama.cpp/vendor/stb:\n",
            "stb_image.h\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mistral_common"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EMiVHc81Ydjb",
        "outputId": "f3160be1-3d56-4a14-d09d-b75fac4dfea5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mistral_common\n",
            "  Downloading mistral_common-1.8.5-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: pydantic<3.0,>=2.7 in /usr/local/lib/python3.12/dist-packages (from mistral_common) (2.11.7)\n",
            "Requirement already satisfied: jsonschema>=4.21.1 in /usr/local/lib/python3.12/dist-packages (from mistral_common) (4.25.1)\n",
            "Requirement already satisfied: typing-extensions>=4.11.0 in /usr/local/lib/python3.12/dist-packages (from mistral_common) (4.15.0)\n",
            "Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from mistral_common) (0.11.0)\n",
            "Requirement already satisfied: pillow>=10.3.0 in /usr/local/lib/python3.12/dist-packages (from mistral_common) (11.3.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from mistral_common) (2.32.4)\n",
            "Requirement already satisfied: numpy>=1.25 in /usr/local/lib/python3.12/dist-packages (from mistral_common) (2.0.2)\n",
            "Collecting pydantic-extra-types>=2.10.5 (from pydantic-extra-types[pycountry]>=2.10.5->mistral_common)\n",
            "  Downloading pydantic_extra_types-2.10.5-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.21.1->mistral_common) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.21.1->mistral_common) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.21.1->mistral_common) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.21.1->mistral_common) (0.27.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0,>=2.7->mistral_common) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0,>=2.7->mistral_common) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0,>=2.7->mistral_common) (0.4.1)\n",
            "Collecting pycountry>=23 (from pydantic-extra-types[pycountry]>=2.10.5->mistral_common)\n",
            "  Downloading pycountry-24.6.1-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->mistral_common) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->mistral_common) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->mistral_common) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->mistral_common) (2025.8.3)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken>=0.7.0->mistral_common) (2024.11.6)\n",
            "Downloading mistral_common-1.8.5-py3-none-any.whl (6.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m72.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_extra_types-2.10.5-py3-none-any.whl (38 kB)\n",
            "Downloading pycountry-24.6.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m127.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pycountry, pydantic-extra-types, mistral_common\n",
            "Successfully installed mistral_common-1.8.5 pycountry-24.6.1 pydantic-extra-types-2.10.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "converter_script_path = \"/content/llama.cpp/convert_hf_to_gguf.py\"\n",
        "\n",
        "import os\n",
        "# This check will stop the script if the path is wrong.\n",
        "if not os.path.exists(converter_script_path):\n",
        "    raise FileNotFoundError(\n",
        "        f\"CRITICAL ERROR: The script was not found at the path you provided: '{converter_script_path}'. \"\n",
        "        \"Please re-run Step 1, find the correct path, and paste it above.\"\n",
        "    )\n",
        "\n",
        "print(f\"✅ Using converter script found at: {converter_script_path}\")\n",
        "print(\"\\n🔄 Converting model to GGUF F16 format...\")\n",
        "\n",
        "# We use the variable to run the script from its confirmed location\n",
        "!python {converter_script_path} llama3_therapist_merged --outfile llama3-therapist.F16.gguf --outtype f16"
      ],
      "metadata": {
        "id": "I5du06gDyD1m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f70dfa68-8708-416a-b6ba-55e41ae7ecca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Using converter script found at: /content/llama.cpp/convert_hf_to_gguf.py\n",
            "\n",
            "🔄 Converting model to GGUF F16 format...\n",
            "INFO:hf-to-gguf:Loading model: llama3_therapist_merged\n",
            "INFO:hf-to-gguf:Model architecture: LlamaForCausalLM\n",
            "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
            "INFO:hf-to-gguf:Exporting model...\n",
            "INFO:hf-to-gguf:gguf: loading model weight map from 'model.safetensors.index.json'\n",
            "INFO:hf-to-gguf:gguf: loading model part 'model-00001-of-00004.safetensors'\n",
            "INFO:hf-to-gguf:token_embd.weight,           torch.float16 --> F16, shape = {4096, 128256}\n",
            "INFO:hf-to-gguf:blk.0.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.0.ffn_down.weight,       torch.float16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.0.ffn_gate.weight,       torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.0.ffn_up.weight,         torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.0.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.0.attn_k.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.0.attn_output.weight,    torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.0.attn_q.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.0.attn_v.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.1.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.1.ffn_down.weight,       torch.float16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.1.ffn_gate.weight,       torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.1.ffn_up.weight,         torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.1.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.1.attn_k.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.1.attn_output.weight,    torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.1.attn_q.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.1.attn_v.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.2.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.2.ffn_down.weight,       torch.float16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.2.ffn_gate.weight,       torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.2.ffn_up.weight,         torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.2.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.2.attn_k.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.2.attn_output.weight,    torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.2.attn_q.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.2.attn_v.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.3.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.3.ffn_down.weight,       torch.float16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.3.ffn_gate.weight,       torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.3.ffn_up.weight,         torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.3.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.3.attn_k.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.3.attn_output.weight,    torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.3.attn_q.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.3.attn_v.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.4.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.4.ffn_down.weight,       torch.float16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.4.ffn_gate.weight,       torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.4.ffn_up.weight,         torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.4.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.4.attn_k.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.4.attn_output.weight,    torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.4.attn_q.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.4.attn_v.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.5.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.5.ffn_down.weight,       torch.float16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.5.ffn_gate.weight,       torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.5.ffn_up.weight,         torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.5.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.5.attn_k.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.5.attn_output.weight,    torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.5.attn_q.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.5.attn_v.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.6.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.6.ffn_down.weight,       torch.float16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.6.ffn_gate.weight,       torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.6.ffn_up.weight,         torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.6.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.6.attn_k.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.6.attn_output.weight,    torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.6.attn_q.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.6.attn_v.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.7.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.7.ffn_down.weight,       torch.float16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.7.ffn_gate.weight,       torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.7.ffn_up.weight,         torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.7.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.7.attn_k.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.7.attn_output.weight,    torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.7.attn_q.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.7.attn_v.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.8.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.8.ffn_down.weight,       torch.float16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.8.ffn_gate.weight,       torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.8.ffn_up.weight,         torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.8.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.8.attn_k.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.8.attn_output.weight,    torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.8.attn_q.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.8.attn_v.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:gguf: loading model part 'model-00002-of-00004.safetensors'\n",
            "INFO:hf-to-gguf:blk.10.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.10.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.10.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.10.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.10.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.10.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.10.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.10.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.10.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.11.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.11.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.11.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.11.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.11.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.11.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.11.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.11.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.11.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.12.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.12.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.12.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.12.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.12.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.12.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.12.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.12.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.12.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.13.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.13.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.13.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.13.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.13.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.13.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.13.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.13.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.13.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.14.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.14.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.14.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.14.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.14.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.14.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.14.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.14.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.14.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.15.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.15.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.15.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.15.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.15.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.15.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.15.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.15.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.15.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.16.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.16.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.16.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.16.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.16.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.16.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.16.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.16.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.16.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.17.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.17.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.17.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.17.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.17.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.17.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.17.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.17.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.17.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.18.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.18.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.18.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.18.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.18.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.18.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.18.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.18.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.18.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.19.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.19.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.19.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.19.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.19.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.19.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.19.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.19.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.19.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.20.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.20.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.20.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.20.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.20.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.9.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.9.ffn_down.weight,       torch.float16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.9.ffn_gate.weight,       torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.9.ffn_up.weight,         torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.9.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.9.attn_k.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.9.attn_output.weight,    torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.9.attn_q.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.9.attn_v.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:gguf: loading model part 'model-00003-of-00004.safetensors'\n",
            "INFO:hf-to-gguf:blk.20.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.20.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.20.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.20.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.21.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.21.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.21.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.21.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.21.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.21.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.21.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.21.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.21.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.22.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.22.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.22.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.22.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.22.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.22.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.22.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.22.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.22.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.23.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.23.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.23.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.23.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.23.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.23.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.23.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.23.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.23.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.24.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.24.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.24.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.24.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.24.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.24.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.24.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.24.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.24.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.25.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.25.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.25.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.25.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.25.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.25.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.25.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.25.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.25.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.26.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.26.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.26.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.26.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.26.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.26.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.26.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.26.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.26.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.27.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.27.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.27.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.27.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.27.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.27.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.27.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.27.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.27.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.28.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.28.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.28.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.28.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.28.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.28.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.28.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.28.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.28.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.29.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.29.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.29.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.29.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.29.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.29.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.29.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.29.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.29.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.30.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.30.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.30.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.30.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.30.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.30.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.30.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.30.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.30.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.31.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.31.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.31.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.31.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.31.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.31.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:gguf: loading model part 'model-00004-of-00004.safetensors'\n",
            "INFO:hf-to-gguf:output.weight,               torch.float16 --> F16, shape = {4096, 128256}\n",
            "INFO:hf-to-gguf:blk.31.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.31.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.31.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:output_norm.weight,          torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:Set meta model\n",
            "INFO:hf-to-gguf:Set model parameters\n",
            "INFO:hf-to-gguf:gguf: context length = 8192\n",
            "INFO:hf-to-gguf:gguf: embedding length = 4096\n",
            "INFO:hf-to-gguf:gguf: feed forward length = 14336\n",
            "INFO:hf-to-gguf:gguf: head count = 32\n",
            "INFO:hf-to-gguf:gguf: key-value head count = 8\n",
            "INFO:hf-to-gguf:gguf: rope theta = 500000.0\n",
            "INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-05\n",
            "INFO:hf-to-gguf:gguf: file type = 1\n",
            "INFO:hf-to-gguf:Set model quantization version\n",
            "INFO:hf-to-gguf:Set model tokenizer\n",
            "INFO:numexpr.utils:NumExpr defaulting to 2 threads.\n",
            "WARNING:gguf.vocab:Unknown separator token '<|begin_of_text|>' in TemplateProcessing<pair>\n",
            "INFO:gguf.vocab:Adding 280147 merge(s).\n",
            "INFO:gguf.vocab:Setting special token type bos to 128000\n",
            "INFO:gguf.vocab:Setting special token type eos to 128009\n",
            "INFO:gguf.vocab:Setting special token type pad to 128255\n",
            "INFO:gguf.vocab:Setting add_bos_token to True\n",
            "INFO:gguf.vocab:Setting add_sep_token to False\n",
            "INFO:gguf.vocab:Setting chat_template to {% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n",
            "\n",
            "'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "' }}{% endif %}\n",
            "INFO:gguf.gguf_writer:Writing the following files:\n",
            "INFO:gguf.gguf_writer:llama3-therapist.F16.gguf: n_tensors = 291, total_size = 16.1G\n",
            "Writing: 100% 16.1G/16.1G [11:05<00:00, 24.1Mbyte/s]\n",
            "INFO:hf-to-gguf:Model successfully exported to llama3-therapist.F16.gguf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quantizing the model"
      ],
      "metadata": {
        "id": "nNfdTeYlWOw6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Clone the repository\n",
        "# !git clone https://github.com/ggerganov/llama.cpp\n",
        "\n",
        "# Build the tools using CMake\n",
        "!cd llama.cpp && mkdir -p build && cd build && cmake .. && cmake --build . --config Release"
      ],
      "metadata": {
        "id": "YBPZpqzWJlfS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1b58a6a-6375-40be-874e-7a7eb4a63556"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-- The C compiler identification is GNU 11.4.0\n",
            "-- The CXX compiler identification is GNU 11.4.0\n",
            "-- Detecting C compiler ABI info\n",
            "-- Detecting C compiler ABI info - done\n",
            "-- Check for working C compiler: /usr/bin/cc - skipped\n",
            "-- Detecting C compile features\n",
            "-- Detecting C compile features - done\n",
            "-- Detecting CXX compiler ABI info\n",
            "-- Detecting CXX compiler ABI info - done\n",
            "-- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
            "-- Detecting CXX compile features\n",
            "-- Detecting CXX compile features - done\n",
            "\u001b[0mCMAKE_BUILD_TYPE=Release\u001b[0m\n",
            "-- Found Git: /usr/bin/git (found version \"2.34.1\")\n",
            "-- The ASM compiler identification is GNU\n",
            "-- Found assembler: /usr/bin/cc\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n",
            "-- Found Threads: TRUE\n",
            "-- Warning: ccache not found - consider installing it for faster compilation or disable this warning with GGML_CCACHE=OFF\n",
            "-- CMAKE_SYSTEM_PROCESSOR: x86_64\n",
            "-- GGML_SYSTEM_ARCH: x86\n",
            "-- Including CPU backend\n",
            "-- Found OpenMP_C: -fopenmp (found version \"4.5\")\n",
            "-- Found OpenMP_CXX: -fopenmp (found version \"4.5\")\n",
            "-- Found OpenMP: TRUE (found version \"4.5\")\n",
            "-- x86 detected\n",
            "-- Adding CPU backend variant ggml-cpu: -march=native \n",
            "-- ggml version: 0.0.1\n",
            "-- ggml commit:  8ff2060\n",
            "-- Found CURL: /usr/lib/x86_64-linux-gnu/libcurl.so (found version \"7.81.0\")\n",
            "-- Configuring done (1.8s)\n",
            "-- Generating done (0.3s)\n",
            "-- Build files have been written to: /content/llama.cpp/build\n",
            "[  1%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o\u001b[0m\n",
            "[  1%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml.cpp.o\u001b[0m\n",
            "[  2%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o\u001b[0m\n",
            "[  2%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o\u001b[0m\n",
            "[  2%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o\u001b[0m\n",
            "[  3%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o\u001b[0m\n",
            "[  3%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o\u001b[0m\n",
            "[  4%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o\u001b[0m\n",
            "[  4%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libggml-base.so\u001b[0m\n",
            "[  4%] Built target ggml-base\n",
            "[  4%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o\u001b[0m\n",
            "[  5%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o\u001b[0m\n",
            "[  5%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/repack.cpp.o\u001b[0m\n",
            "[  6%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/hbm.cpp.o\u001b[0m\n",
            "[  6%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/quants.c.o\u001b[0m\n",
            "[  6%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/traits.cpp.o\u001b[0m\n",
            "[  7%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o\u001b[0m\n",
            "[  7%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o\u001b[0m\n",
            "[  8%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/binary-ops.cpp.o\u001b[0m\n",
            "[  8%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/unary-ops.cpp.o\u001b[0m\n",
            "[  8%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/vec.cpp.o\u001b[0m\n",
            "[  9%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ops.cpp.o\u001b[0m\n",
            "[  9%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o\u001b[0m\n",
            "[ 10%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/quants.c.o\u001b[0m\n",
            "[ 10%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/repack.cpp.o\u001b[0m\n",
            "[ 10%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libggml-cpu.so\u001b[0m\n",
            "[ 10%] Built target ggml-cpu\n",
            "[ 10%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o\u001b[0m\n",
            "[ 10%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libggml.so\u001b[0m\n",
            "[ 10%] Built target ggml\n",
            "[ 11%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama.cpp.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-cparams.cpp.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-graph.cpp.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-io.cpp.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-kv-cache-iswa.cpp.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-memory.cpp.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-memory-hybrid.cpp.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-memory-recurrent.cpp.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-model-saver.cpp.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/unicode.cpp.o\u001b[0m\n",
            "[ 21%] \u001b[32m\u001b[1mLinking CXX shared library ../bin/libllama.so\u001b[0m\n",
            "[ 21%] Built target llama\n",
            "[ 21%] \u001b[32mBuilding CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o\u001b[0m\n",
            "[ 21%] Built target build_info\n",
            "[ 21%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/arg.cpp.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/chat-parser.cpp.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/chat.cpp.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/common.cpp.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/console.cpp.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/json-partial.cpp.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/llguidance.cpp.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/log.cpp.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/regex-partial.cpp.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/sampling.cpp.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/speculative.cpp.o\u001b[0m\n",
            "[ 27%] \u001b[32m\u001b[1mLinking CXX static library libcommon.a\u001b[0m\n",
            "[ 27%] Built target common\n",
            "[ 27%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o\u001b[0m\n",
            "[ 27%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-0\u001b[0m\n",
            "[ 27%] Built target test-tokenizer-0\n",
            "[ 28%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o\u001b[0m\n",
            "[ 28%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o\u001b[0m\n",
            "[ 28%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-sampling\u001b[0m\n",
            "[ 28%] Built target test-sampling\n",
            "[ 28%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o\u001b[0m\n",
            "[ 29%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o\u001b[0m\n",
            "[ 29%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-grammar-parser\u001b[0m\n",
            "[ 29%] Built target test-grammar-parser\n",
            "[ 29%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o\u001b[0m\n",
            "[ 30%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-grammar-integration\u001b[0m\n",
            "[ 30%] Built target test-grammar-integration\n",
            "[ 30%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o\u001b[0m\n",
            "[ 31%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-llama-grammar\u001b[0m\n",
            "[ 31%] Built target test-llama-grammar\n",
            "[ 32%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat.dir/test-chat.cpp.o\u001b[0m\n",
            "[ 32%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat.dir/get-model.cpp.o\u001b[0m\n",
            "[ 32%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-chat\u001b[0m\n",
            "[ 32%] Built target test-chat\n",
            "[ 33%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o\u001b[0m\n",
            "[ 33%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o\u001b[0m\n",
            "[ 34%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-json-schema-to-grammar\u001b[0m\n",
            "[ 34%] Built target test-json-schema-to-grammar\n",
            "[ 34%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-stats.dir/test-quantize-stats.cpp.o\u001b[0m\n",
            "[ 35%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-quantize-stats\u001b[0m\n",
            "[ 35%] Built target test-quantize-stats\n",
            "[ 35%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-gbnf-validator.dir/test-gbnf-validator.cpp.o\u001b[0m\n",
            "[ 36%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-gbnf-validator\u001b[0m\n",
            "[ 36%] Built target test-gbnf-validator\n",
            "[ 37%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o\u001b[0m\n",
            "[ 37%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-1-bpe\u001b[0m\n",
            "[ 37%] Built target test-tokenizer-1-bpe\n",
            "[ 38%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o\u001b[0m\n",
            "[ 38%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-1-spm\u001b[0m\n",
            "[ 38%] Built target test-tokenizer-1-spm\n",
            "[ 39%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-parser.dir/test-chat-parser.cpp.o\u001b[0m\n",
            "[ 39%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-parser.dir/get-model.cpp.o\u001b[0m\n",
            "[ 40%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-chat-parser\u001b[0m\n",
            "[ 40%] Built target test-chat-parser\n",
            "[ 40%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o\u001b[0m\n",
            "[ 40%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o\u001b[0m\n",
            "[ 41%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-chat-template\u001b[0m\n",
            "[ 41%] Built target test-chat-template\n",
            "[ 42%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-partial.dir/test-json-partial.cpp.o\u001b[0m\n",
            "[ 42%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-partial.dir/get-model.cpp.o\u001b[0m\n",
            "[ 42%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-json-partial\u001b[0m\n",
            "[ 42%] Built target test-json-partial\n",
            "[ 42%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o\u001b[0m\n",
            "[ 43%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o\u001b[0m\n",
            "[ 43%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-log\u001b[0m\n",
            "[ 43%] Built target test-log\n",
            "[ 43%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-regex-partial.dir/test-regex-partial.cpp.o\u001b[0m\n",
            "[ 44%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-regex-partial.dir/get-model.cpp.o\u001b[0m\n",
            "[ 44%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-regex-partial\u001b[0m\n",
            "[ 44%] Built target test-regex-partial\n",
            "[ 45%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-thread-safety.dir/test-thread-safety.cpp.o\u001b[0m\n",
            "[ 45%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-thread-safety.dir/get-model.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-thread-safety\u001b[0m\n",
            "[ 46%] Built target test-thread-safety\n",
            "[ 46%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-arg-parser\u001b[0m\n",
            "[ 47%] Built target test-arg-parser\n",
            "[ 48%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-opt.dir/test-opt.cpp.o\u001b[0m\n",
            "[ 48%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-opt.dir/get-model.cpp.o\u001b[0m\n",
            "[ 49%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-opt\u001b[0m\n",
            "[ 49%] Built target test-opt\n",
            "[ 49%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o\u001b[0m\n",
            "[ 49%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o\u001b[0m\n",
            "[ 50%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-gguf\u001b[0m\n",
            "[ 50%] Built target test-gguf\n",
            "[ 50%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o\u001b[0m\n",
            "[ 51%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o\u001b[0m\n",
            "[ 51%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-backend-ops\u001b[0m\n",
            "[ 51%] Built target test-backend-ops\n",
            "[ 51%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o\u001b[0m\n",
            "[ 52%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o\u001b[0m\n",
            "[ 52%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-model-load-cancel\u001b[0m\n",
            "[ 52%] Built target test-model-load-cancel\n",
            "[ 52%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o\u001b[0m\n",
            "[ 53%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o\u001b[0m\n",
            "[ 53%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-autorelease\u001b[0m\n",
            "[ 53%] Built target test-autorelease\n",
            "[ 54%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o\u001b[0m\n",
            "[ 54%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o\u001b[0m\n",
            "[ 54%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-barrier\u001b[0m\n",
            "[ 54%] Built target test-barrier\n",
            "[ 54%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o\u001b[0m\n",
            "[ 54%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o\u001b[0m\n",
            "[ 55%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-quantize-fns\u001b[0m\n",
            "[ 55%] Built target test-quantize-fns\n",
            "[ 55%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o\u001b[0m\n",
            "[ 56%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o\u001b[0m\n",
            "[ 56%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-quantize-perf\u001b[0m\n",
            "[ 56%] Built target test-quantize-perf\n",
            "[ 56%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o\u001b[0m\n",
            "[ 57%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o\u001b[0m\n",
            "[ 57%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-rope\u001b[0m\n",
            "[ 57%] Built target test-rope\n",
            "[ 57%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/mtmd.cpp.o\u001b[0m\n",
            "[ 58%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/mtmd-audio.cpp.o\u001b[0m\n",
            "[ 58%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/clip.cpp.o\u001b[0m\n",
            "[ 58%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/mtmd-helper.cpp.o\u001b[0m\n",
            "[ 59%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libmtmd.so\u001b[0m\n",
            "[ 59%] Built target mtmd\n",
            "[ 60%] \u001b[32mBuilding C object tests/CMakeFiles/test-mtmd-c-api.dir/test-mtmd-c-api.c.o\u001b[0m\n",
            "[ 60%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-mtmd-c-api.dir/get-model.cpp.o\u001b[0m\n",
            "[ 60%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-mtmd-c-api\u001b[0m\n",
            "[ 60%] Built target test-mtmd-c-api\n",
            "[ 61%] \u001b[32mBuilding C object tests/CMakeFiles/test-c.dir/test-c.c.o\u001b[0m\n",
            "[ 61%] \u001b[32m\u001b[1mLinking C executable ../bin/test-c\u001b[0m\n",
            "[ 61%] Built target test-c\n",
            "[ 62%] \u001b[32mBuilding CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o\u001b[0m\n",
            "[ 62%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-batched\u001b[0m\n",
            "[ 62%] Built target llama-batched\n",
            "[ 62%] \u001b[32mBuilding CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o\u001b[0m\n",
            "[ 63%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-embedding\u001b[0m\n",
            "[ 63%] Built target llama-embedding\n",
            "[ 63%] \u001b[32mBuilding CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o\u001b[0m\n",
            "[ 63%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-eval-callback\u001b[0m\n",
            "[ 63%] Built target llama-eval-callback\n",
            "[ 64%] \u001b[32mBuilding C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o\u001b[0m\n",
            "[ 64%] Built target sha256\n",
            "[ 65%] \u001b[32mBuilding C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o\u001b[0m\n",
            "[ 65%] Built target xxhash\n",
            "[ 65%] \u001b[32mBuilding C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o\u001b[0m\n",
            "[ 65%] Built target sha1\n",
            "[ 66%] \u001b[32mBuilding CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o\u001b[0m\n",
            "[ 66%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gguf-hash\u001b[0m\n",
            "[ 66%] Built target llama-gguf-hash\n",
            "[ 66%] \u001b[32mBuilding CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o\u001b[0m\n",
            "[ 66%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gguf\u001b[0m\n",
            "[ 66%] Built target llama-gguf\n",
            "[ 66%] \u001b[32mBuilding CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o\u001b[0m\n",
            "[ 67%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gritlm\u001b[0m\n",
            "[ 67%] Built target llama-gritlm\n",
            "[ 68%] \u001b[32mBuilding CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o\u001b[0m\n",
            "[ 68%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookahead\u001b[0m\n",
            "[ 68%] Built target llama-lookahead\n",
            "[ 68%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o\u001b[0m\n",
            "[ 69%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup\u001b[0m\n",
            "[ 69%] Built target llama-lookup\n",
            "[ 69%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o\u001b[0m\n",
            "[ 70%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup-create\u001b[0m\n",
            "[ 70%] Built target llama-lookup-create\n",
            "[ 70%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o\u001b[0m\n",
            "[ 70%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup-merge\u001b[0m\n",
            "[ 70%] Built target llama-lookup-merge\n",
            "[ 71%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o\u001b[0m\n",
            "[ 71%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup-stats\u001b[0m\n",
            "[ 71%] Built target llama-lookup-stats\n",
            "[ 71%] \u001b[32mBuilding CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o\u001b[0m\n",
            "[ 72%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-parallel\u001b[0m\n",
            "[ 72%] Built target llama-parallel\n",
            "[ 72%] \u001b[32mBuilding CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o\u001b[0m\n",
            "[ 73%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-passkey\u001b[0m\n",
            "[ 73%] Built target llama-passkey\n",
            "[ 73%] \u001b[32mBuilding CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o\u001b[0m\n",
            "[ 74%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-retrieval\u001b[0m\n",
            "[ 74%] Built target llama-retrieval\n",
            "[ 74%] \u001b[32mBuilding CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o\u001b[0m\n",
            "[ 75%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-save-load-state\u001b[0m\n",
            "[ 75%] Built target llama-save-load-state\n",
            "[ 76%] \u001b[32mBuilding CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o\u001b[0m\n",
            "[ 76%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-simple\u001b[0m\n",
            "[ 76%] Built target llama-simple\n",
            "[ 76%] \u001b[32mBuilding CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o\u001b[0m\n",
            "[ 77%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-simple-chat\u001b[0m\n",
            "[ 77%] Built target llama-simple-chat\n",
            "[ 77%] \u001b[32mBuilding CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o\u001b[0m\n",
            "[ 78%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-speculative\u001b[0m\n",
            "[ 78%] Built target llama-speculative\n",
            "[ 78%] \u001b[32mBuilding CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o\u001b[0m\n",
            "[ 78%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-speculative-simple\u001b[0m\n",
            "[ 78%] Built target llama-speculative-simple\n",
            "[ 78%] \u001b[32mBuilding CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o\u001b[0m\n",
            "[ 79%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gen-docs\u001b[0m\n",
            "[ 79%] Built target llama-gen-docs\n",
            "[ 80%] \u001b[32mBuilding CXX object examples/training/CMakeFiles/llama-finetune.dir/finetune.cpp.o\u001b[0m\n",
            "[ 80%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-finetune\u001b[0m\n",
            "[ 80%] Built target llama-finetune\n",
            "[ 80%] \u001b[32mBuilding CXX object examples/diffusion/CMakeFiles/llama-diffusion-cli.dir/diffusion-cli.cpp.o\u001b[0m\n",
            "[ 81%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-diffusion-cli\u001b[0m\n",
            "[ 81%] Built target llama-diffusion-cli\n",
            "[ 82%] \u001b[32mBuilding CXX object examples/model-conversion/CMakeFiles/llama-logits.dir/logits.cpp.o\u001b[0m\n",
            "[ 82%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-logits\u001b[0m\n",
            "[ 82%] Built target llama-logits\n",
            "[ 83%] \u001b[32mBuilding CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o\u001b[0m\n",
            "[ 83%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-convert-llama2c-to-ggml\u001b[0m\n",
            "[ 83%] Built target llama-convert-llama2c-to-ggml\n",
            "[ 83%] \u001b[32mBuilding CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o\u001b[0m\n",
            "[ 84%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-vdot\u001b[0m\n",
            "[ 84%] Built target llama-vdot\n",
            "[ 85%] \u001b[32mBuilding CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o\u001b[0m\n",
            "[ 85%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-q8dot\u001b[0m\n",
            "[ 85%] Built target llama-q8dot\n",
            "[ 85%] \u001b[32mBuilding CXX object tools/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o\u001b[0m\n",
            "[ 86%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-batched-bench\u001b[0m\n",
            "[ 86%] Built target llama-batched-bench\n",
            "[ 87%] \u001b[32mBuilding CXX object tools/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o\u001b[0m\n",
            "[ 87%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gguf-split\u001b[0m\n",
            "[ 87%] Built target llama-gguf-split\n",
            "[ 87%] \u001b[32mBuilding CXX object tools/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o\u001b[0m\n",
            "[ 88%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-imatrix\u001b[0m\n",
            "[ 88%] Built target llama-imatrix\n",
            "[ 88%] \u001b[32mBuilding CXX object tools/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o\u001b[0m\n",
            "[ 89%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-bench\u001b[0m\n",
            "[ 89%] Built target llama-bench\n",
            "[ 89%] \u001b[32mBuilding CXX object tools/main/CMakeFiles/llama-cli.dir/main.cpp.o\u001b[0m\n",
            "[ 89%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-cli\u001b[0m\n",
            "[ 89%] Built target llama-cli\n",
            "[ 89%] \u001b[32mBuilding CXX object tools/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o\u001b[0m\n",
            "[ 89%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-perplexity\u001b[0m\n",
            "[ 89%] Built target llama-perplexity\n",
            "[ 90%] \u001b[32mBuilding CXX object tools/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o\u001b[0m\n",
            "[ 90%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-quantize\u001b[0m\n",
            "[ 90%] Built target llama-quantize\n",
            "[ 90%] \u001b[34m\u001b[1mGenerating loading.html.hpp\u001b[0m\n",
            "[ 90%] \u001b[34m\u001b[1mGenerating index.html.gz.hpp\u001b[0m\n",
            "[ 91%] \u001b[32mBuilding CXX object tools/server/CMakeFiles/llama-server.dir/server.cpp.o\u001b[0m\n",
            "[ 91%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-server\u001b[0m\n",
            "[ 91%] Built target llama-server\n",
            "[ 91%] \u001b[32mBuilding CXX object tools/run/CMakeFiles/llama-run.dir/run.cpp.o\u001b[0m\n",
            "[ 91%] \u001b[32mBuilding CXX object tools/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o\u001b[0m\n",
            "[ 92%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-run\u001b[0m\n",
            "[ 92%] Built target llama-run\n",
            "[ 93%] \u001b[32mBuilding CXX object tools/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o\u001b[0m\n",
            "[ 93%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-tokenize\u001b[0m\n",
            "[ 93%] Built target llama-tokenize\n",
            "[ 94%] \u001b[32mBuilding CXX object tools/tts/CMakeFiles/llama-tts.dir/tts.cpp.o\u001b[0m\n",
            "[ 94%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-tts\u001b[0m\n",
            "[ 94%] Built target llama-tts\n",
            "[ 94%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-llava-cli.dir/deprecation-warning.cpp.o\u001b[0m\n",
            "[ 94%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-llava-cli\u001b[0m\n",
            "[ 94%] Built target llama-llava-cli\n",
            "[ 94%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-gemma3-cli.dir/deprecation-warning.cpp.o\u001b[0m\n",
            "[ 95%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gemma3-cli\u001b[0m\n",
            "[ 95%] Built target llama-gemma3-cli\n",
            "[ 96%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-minicpmv-cli.dir/deprecation-warning.cpp.o\u001b[0m\n",
            "[ 96%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-minicpmv-cli\u001b[0m\n",
            "[ 96%] Built target llama-minicpmv-cli\n",
            "[ 96%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-qwen2vl-cli.dir/deprecation-warning.cpp.o\u001b[0m\n",
            "[ 97%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-qwen2vl-cli\u001b[0m\n",
            "[ 97%] Built target llama-qwen2vl-cli\n",
            "[ 97%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-mtmd-cli.dir/mtmd-cli.cpp.o\u001b[0m\n",
            "[ 98%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-mtmd-cli\u001b[0m\n",
            "[ 98%] Built target llama-mtmd-cli\n",
            "[ 99%] \u001b[32mBuilding CXX object tools/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o\u001b[0m\n",
            "[ 99%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-cvector-generator\u001b[0m\n",
            "[ 99%] Built target llama-cvector-generator\n",
            "[100%] \u001b[32mBuilding CXX object tools/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o\u001b[0m\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-export-lora\u001b[0m\n",
            "[100%] Built target llama-export-lora\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This command takes the 16 GB file as input and outputs the final ~4 GB file\n",
        "!./llama.cpp/build/bin/llama-quantize ./llama3-therapist.F16.gguf ./llama3-therapist.Q4_K_M.gguf q4_k_m"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3LxL1SAcWTGl",
        "outputId": "71db7dfc-cec8-4c7c-98ba-5b174a74f7c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "main: build = 1 (8ff2060)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04.2) 11.4.0 for x86_64-linux-gnu\n",
            "main: quantizing './llama3-therapist.F16.gguf' to './llama3-therapist.Q4_K_M.gguf' as Q4_K_M\n",
            "llama_model_loader: loaded meta data with 29 key-value pairs and 291 tensors from ./llama3-therapist.F16.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Llama3_Therapist_Merged\n",
            "llama_model_loader: - kv   3:                         general.size_label str              = 8.0B\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   5:                       llama.context_length u32              = 8192\n",
            "llama_model_loader: - kv   6:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   7:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 500000.000000\n",
            "llama_model_loader: - kv  11:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  12:                 llama.attention.key_length u32              = 128\n",
            "llama_model_loader: - kv  13:               llama.attention.value_length u32              = 128\n",
            "llama_model_loader: - kv  14:                          general.file_type u32              = 1\n",
            "llama_model_loader: - kv  15:                           llama.vocab_size u32              = 128256\n",
            "llama_model_loader: - kv  16:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  17:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  18:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  19:                         tokenizer.ggml.pre str              = llama-bpe\n",
            "llama_model_loader: - kv  20:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  21:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  22:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
            "llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 128000\n",
            "llama_model_loader: - kv  24:                tokenizer.ggml.eos_token_id u32              = 128009\n",
            "llama_model_loader: - kv  25:            tokenizer.ggml.padding_token_id u32              = 128255\n",
            "llama_model_loader: - kv  26:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  27:               tokenizer.ggml.add_sep_token bool             = false\n",
            "llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type  f16:  226 tensors\n",
            "[   1/ 291]                        output.weight - [ 4096, 128256,     1,     1], type =    f16, converting to q6_K .. size =  1002.00 MiB ->   410.98 MiB\n",
            "[   2/ 291]                   output_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
            "[   3/ 291]                    token_embd.weight - [ 4096, 128256,     1,     1], type =    f16, converting to q4_K .. size =  1002.00 MiB ->   281.81 MiB\n",
            "[   4/ 291]                  blk.0.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[   5/ 291]               blk.0.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
            "[   6/ 291]             blk.0.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[   7/ 291]                  blk.0.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[   8/ 291]                  blk.0.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[   9/ 291]                blk.0.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[  10/ 291]                blk.0.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  11/ 291]                blk.0.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
            "[  12/ 291]                  blk.0.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  13/ 291]                  blk.1.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  14/ 291]               blk.1.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
            "[  15/ 291]             blk.1.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  16/ 291]                  blk.1.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  17/ 291]                  blk.1.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[  18/ 291]                blk.1.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[  19/ 291]                blk.1.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  20/ 291]                blk.1.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
            "[  21/ 291]                  blk.1.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  22/ 291]                  blk.2.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  23/ 291]               blk.2.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
            "[  24/ 291]             blk.2.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  25/ 291]                  blk.2.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  26/ 291]                  blk.2.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[  27/ 291]                blk.2.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[  28/ 291]                blk.2.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  29/ 291]                blk.2.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
            "[  30/ 291]                  blk.2.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  31/ 291]                  blk.3.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  32/ 291]               blk.3.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
            "[  33/ 291]             blk.3.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  34/ 291]                  blk.3.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  35/ 291]                  blk.3.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[  36/ 291]                blk.3.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[  37/ 291]                blk.3.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  38/ 291]                blk.3.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
            "[  39/ 291]                  blk.3.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  40/ 291]                  blk.4.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  41/ 291]               blk.4.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
            "[  42/ 291]             blk.4.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  43/ 291]                  blk.4.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  44/ 291]                  blk.4.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  45/ 291]                blk.4.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  46/ 291]                blk.4.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  47/ 291]                blk.4.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
            "[  48/ 291]                  blk.4.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  49/ 291]                  blk.5.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  50/ 291]               blk.5.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
            "[  51/ 291]             blk.5.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  52/ 291]                  blk.5.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  53/ 291]                  blk.5.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  54/ 291]                blk.5.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  55/ 291]                blk.5.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  56/ 291]                blk.5.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
            "[  57/ 291]                  blk.5.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  58/ 291]                  blk.6.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  59/ 291]               blk.6.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
            "[  60/ 291]             blk.6.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  61/ 291]                  blk.6.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  62/ 291]                  blk.6.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[  63/ 291]                blk.6.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[  64/ 291]                blk.6.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  65/ 291]                blk.6.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
            "[  66/ 291]                  blk.6.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  67/ 291]                  blk.7.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  68/ 291]               blk.7.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
            "[  69/ 291]             blk.7.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  70/ 291]                  blk.7.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  71/ 291]                  blk.7.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  72/ 291]                blk.7.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  73/ 291]                blk.7.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  74/ 291]                blk.7.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
            "[  75/ 291]                  blk.7.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  76/ 291]                  blk.8.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  77/ 291]               blk.8.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
            "[  78/ 291]             blk.8.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  79/ 291]                  blk.8.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  80/ 291]                  blk.8.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  81/ 291]                blk.8.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  82/ 291]                blk.8.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  83/ 291]                blk.8.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
            "[  84/ 291]                  blk.8.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  85/ 291]                  blk.9.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  86/ 291]               blk.9.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
            "[  87/ 291]             blk.9.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  88/ 291]                  blk.9.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  89/ 291]                  blk.9.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[  90/ 291]                blk.9.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[  91/ 291]                blk.9.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  92/ 291]                blk.9.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
            "[  93/ 291]                  blk.9.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  94/ 291]                 blk.10.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  95/ 291]              blk.10.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
            "[  96/ 291]            blk.10.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  97/ 291]                 blk.10.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  98/ 291]                 blk.10.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  99/ 291]               blk.10.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 100/ 291]               blk.10.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 101/ 291]               blk.10.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
            "[ 102/ 291]                 blk.10.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 103/ 291]                 blk.11.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 104/ 291]              blk.11.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
            "[ 105/ 291]            blk.11.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 106/ 291]                 blk.11.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 107/ 291]                 blk.11.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 108/ 291]               blk.11.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 109/ 291]               blk.11.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 110/ 291]               blk.11.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
            "[ 111/ 291]                 blk.11.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 112/ 291]                 blk.12.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 113/ 291]              blk.12.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
            "[ 114/ 291]            blk.12.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 115/ 291]                 blk.12.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 116/ 291]                 blk.12.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 117/ 291]               blk.12.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 118/ 291]               blk.12.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 119/ 291]               blk.12.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
            "[ 120/ 291]                 blk.12.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 121/ 291]                 blk.13.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 122/ 291]              blk.13.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
            "[ 123/ 291]            blk.13.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 124/ 291]                 blk.13.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 125/ 291]                 blk.13.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 126/ 291]               blk.13.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 127/ 291]               blk.13.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 128/ 291]               blk.13.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
            "[ 129/ 291]                 blk.13.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 130/ 291]                 blk.14.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 131/ 291]              blk.14.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
            "[ 132/ 291]            blk.14.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 133/ 291]                 blk.14.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 134/ 291]                 blk.14.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 135/ 291]               blk.14.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 136/ 291]               blk.14.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 137/ 291]               blk.14.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
            "[ 138/ 291]                 blk.14.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 139/ 291]                 blk.15.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 140/ 291]              blk.15.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
            "[ 141/ 291]            blk.15.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 142/ 291]                 blk.15.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 143/ 291]                 blk.15.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 144/ 291]               blk.15.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 145/ 291]               blk.15.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 146/ 291]               blk.15.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
            "[ 147/ 291]                 blk.15.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 148/ 291]                 blk.16.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 149/ 291]              blk.16.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
            "[ 150/ 291]            blk.16.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 151/ 291]                 blk.16.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 152/ 291]                 blk.16.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 153/ 291]               blk.16.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 154/ 291]               blk.16.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 155/ 291]               blk.16.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
            "[ 156/ 291]                 blk.16.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 157/ 291]                 blk.17.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 158/ 291]              blk.17.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
            "[ 159/ 291]            blk.17.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 160/ 291]                 blk.17.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 161/ 291]                 blk.17.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 162/ 291]               blk.17.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 163/ 291]               blk.17.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 164/ 291]               blk.17.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
            "[ 165/ 291]                 blk.17.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 166/ 291]                 blk.18.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 167/ 291]              blk.18.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
            "[ 168/ 291]            blk.18.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 169/ 291]                 blk.18.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 170/ 291]                 blk.18.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 171/ 291]               blk.18.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 172/ 291]               blk.18.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 173/ 291]               blk.18.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
            "[ 174/ 291]                 blk.18.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 175/ 291]                 blk.19.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 176/ 291]              blk.19.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
            "[ 177/ 291]            blk.19.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 178/ 291]                 blk.19.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 179/ 291]                 blk.19.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 180/ 291]               blk.19.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 181/ 291]               blk.19.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 182/ 291]               blk.19.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
            "[ 183/ 291]                 blk.19.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 184/ 291]                 blk.20.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 185/ 291]              blk.20.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
            "[ 186/ 291]            blk.20.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 187/ 291]                 blk.20.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 188/ 291]                 blk.20.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 189/ 291]               blk.20.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 190/ 291]               blk.20.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 191/ 291]               blk.20.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
            "[ 192/ 291]                 blk.20.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 193/ 291]                 blk.21.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 194/ 291]              blk.21.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
            "[ 195/ 291]            blk.21.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 196/ 291]                 blk.21.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 197/ 291]                 blk.21.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 198/ 291]               blk.21.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 199/ 291]               blk.21.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 200/ 291]               blk.21.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
            "[ 201/ 291]                 blk.21.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 202/ 291]                 blk.22.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 203/ 291]              blk.22.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
            "[ 204/ 291]            blk.22.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 205/ 291]                 blk.22.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 206/ 291]                 blk.22.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 207/ 291]               blk.22.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 208/ 291]               blk.22.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 209/ 291]               blk.22.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
            "[ 210/ 291]                 blk.22.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 211/ 291]                 blk.23.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 212/ 291]              blk.23.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
            "[ 213/ 291]            blk.23.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 214/ 291]                 blk.23.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 215/ 291]                 blk.23.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 216/ 291]               blk.23.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 217/ 291]               blk.23.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 218/ 291]               blk.23.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
            "[ 219/ 291]                 blk.23.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 220/ 291]                 blk.24.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 221/ 291]              blk.24.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
            "[ 222/ 291]            blk.24.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 223/ 291]                 blk.24.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 224/ 291]                 blk.24.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 225/ 291]               blk.24.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 226/ 291]               blk.24.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 227/ 291]               blk.24.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
            "[ 228/ 291]                 blk.24.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 229/ 291]                 blk.25.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 230/ 291]              blk.25.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
            "[ 231/ 291]            blk.25.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 232/ 291]                 blk.25.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 233/ 291]                 blk.25.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 234/ 291]               blk.25.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 235/ 291]               blk.25.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 236/ 291]               blk.25.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
            "[ 237/ 291]                 blk.25.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 238/ 291]                 blk.26.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 239/ 291]              blk.26.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
            "[ 240/ 291]            blk.26.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 241/ 291]                 blk.26.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 242/ 291]                 blk.26.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 243/ 291]               blk.26.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 244/ 291]               blk.26.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 245/ 291]               blk.26.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
            "[ 246/ 291]                 blk.26.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 247/ 291]                 blk.27.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 248/ 291]              blk.27.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
            "[ 249/ 291]            blk.27.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 250/ 291]                 blk.27.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 251/ 291]                 blk.27.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 252/ 291]               blk.27.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 253/ 291]               blk.27.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 254/ 291]               blk.27.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
            "[ 255/ 291]                 blk.27.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 256/ 291]                 blk.28.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 257/ 291]              blk.28.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
            "[ 258/ 291]            blk.28.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 259/ 291]                 blk.28.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 260/ 291]                 blk.28.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 261/ 291]               blk.28.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 262/ 291]               blk.28.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 263/ 291]               blk.28.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
            "[ 264/ 291]                 blk.28.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 265/ 291]                 blk.29.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 266/ 291]              blk.29.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
            "[ 267/ 291]            blk.29.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 268/ 291]                 blk.29.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 269/ 291]                 blk.29.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 270/ 291]               blk.29.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 271/ 291]               blk.29.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 272/ 291]               blk.29.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
            "[ 273/ 291]                 blk.29.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 274/ 291]                 blk.30.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 275/ 291]              blk.30.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
            "[ 276/ 291]            blk.30.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 277/ 291]                 blk.30.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 278/ 291]                 blk.30.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 279/ 291]               blk.30.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 280/ 291]               blk.30.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 281/ 291]               blk.30.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
            "[ 282/ 291]                 blk.30.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 283/ 291]                 blk.31.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 284/ 291]              blk.31.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
            "[ 285/ 291]            blk.31.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 286/ 291]                 blk.31.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 287/ 291]                 blk.31.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 288/ 291]               blk.31.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 289/ 291]               blk.31.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 290/ 291]               blk.31.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MiB\n",
            "[ 291/ 291]                 blk.31.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "llama_model_quantize_impl: model size  = 15317.02 MiB\n",
            "llama_model_quantize_impl: quant size  =  4685.30 MiB\n",
            "\n",
            "main: quantize time = 1392066.22 ms\n",
            "main:    total time = 1392066.22 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-ufSAUqzeSUr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}